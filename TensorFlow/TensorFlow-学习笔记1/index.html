<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay tuned"><title>TensorFlow 学习笔记1 | 大腿成长记</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TensorFlow 学习笔记1</h1><a id="logo" href="/.">大腿成长记</a><p class="description">Keep my dream alive, what if it come true?</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">TensorFlow 学习笔记1</h1><div class="post-meta">Apr 28, 2017<span> | </span><span class="category"><a href="/categories/TensorFlow/">TensorFlow</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="TensorFlow/TensorFlow-学习笔记1/" href="/TensorFlow/TensorFlow-学习笔记1/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basics-of-TensorFlow"><span class="toc-number">1.</span> <span class="toc-text">Basics of TensorFlow</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Function-in-TF"><span class="toc-number">2.</span> <span class="toc-text">Linear Function in TF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Initialization"><span class="toc-number">2.1.</span> <span class="toc-text">Initialization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax"><span class="toc-number">3.</span> <span class="toc-text">Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Entropy"><span class="toc-number">4.</span> <span class="toc-text">Cross Entropy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch"><span class="toc-number">4.1.</span> <span class="toc-text">Mini-batch</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Calculate-Accuracy"><span class="toc-number">5.</span> <span class="toc-text">Calculate Accuracy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Activation"><span class="toc-number">6.</span> <span class="toc-text">Activation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MNIST-example"><span class="toc-number">7.</span> <span class="toc-text">MNIST example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Save-and-Restore-TensorFlow-Models"><span class="toc-number">8.</span> <span class="toc-text">Save and Restore TensorFlow Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Save-a-Trained-Model"><span class="toc-number">8.1.</span> <span class="toc-text">Save a Trained Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Load-a-Trained-Model"><span class="toc-number">8.2.</span> <span class="toc-text">Load a Trained Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-Tuning"><span class="toc-number">9.</span> <span class="toc-text">Fine Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization"><span class="toc-number">10.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow-vs-Numpy"><span class="toc-number">11.</span> <span class="toc-text">TensorFlow vs Numpy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TF-Session-Object"><span class="toc-number">12.</span> <span class="toc-text">TF Session Object</span></a></li></ol></div></div><div class="post-content"><p>基本的TF functions 覆盖了NN的基本的流程，initialization, activation function, regularization, 并以经典的MNIST 为例走一遍。先define model，不变的比如input，labels 设为<code>tf.placeholder()</code>, 需要update的比如weights和bias设为<code>tf.Variables()</code>， 然后定义calculation，最后用<code>tf.Session().run()</code>计算，赋值需要用到<code>feed_dict</code></p>
<h2 id="Basics-of-TensorFlow"><a href="#Basics-of-TensorFlow" class="headerlink" title="Basics of TensorFlow"></a>Basics of TensorFlow</h2><p>Ran operations in<code>tf.Session</code></p>
<p>Create a constant tensor <code>tf.constant()</code></p>
<p>Get input <code>tf.placeholder()</code> 用于函数run里定义and <code>feed_dict</code>用于tf.Session.run()赋值</p>
<p>定义变量 <code>tf.Variable()</code></p>
<p>Basic math <code>tf.add(), tf.subtract(), tf.multiply(), tf.divide()</code></p>
<p>Convert data type <code>tf.cast()</code></p>
<h2 id="Linear-Function-in-TF"><a href="#Linear-Function-in-TF" class="headerlink" title="Linear Function in TF"></a>Linear Function in TF</h2><p>Inputs, matrix of the weights and biases</p>
<center>$$ y = xW+b$$</center>

<p>Weights $W$ and bias $b$ need to be a Tensor that can be modified-&gt; <code>tf.Variable</code></p>
<p>Matrix multiplication <code>tf.matmul()</code>, orders matter!</p>
<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>TensorFlow variables must be initialized before they have values.</p>
<p><code>tf.global_variables_initializer()</code> will initialize all TF variables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(init)</div></pre></td></tr></table></figure>
<p>Initialize the random weight from a normal distribution<code>tf.truncated_normal()</code> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">n_features = <span class="number">120</span></div><div class="line">n_labels = <span class="number">5</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([n_features, n_labels]))</div></pre></td></tr></table></figure>
<p>Initialize the bias to all zeros</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bias=tf.Variable(tf.zeros(n_labels))</div></pre></td></tr></table></figure>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>The softmax function squashes it’s inputs, typically called <strong>logits</strong> or <strong>logit scores</strong>, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></div><div class="line">	output = <span class="keyword">None</span></div><div class="line">    logit_data = [<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]</div><div class="line">  	logits = tf.placeholder(tf.float32)</div><div class="line">  	softmax = tf.nn.softmax(logits)</div><div class="line">    </div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        output = sess.run(softmax, feed_dict=&#123;logis: logit_data&#125;)</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff3v4f4isrj30uw0dodgz.jpg" alt="cross entropy loss function"></p>
<p>Sum function<code>tf.reduce_sum()</code> and log function <code>tf.log()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">crossentropy = -tf.reduce_sum(tf.multiply(tf.log(softmax), one_hot))</div></pre></td></tr></table></figure>
<h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><p> Training on subsets of the dataset instead of all the data at one time.</p>
<ol>
<li><p>Divide the data into batches</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Features and Labels</span></div><div class="line">features = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input])</div><div class="line">labels = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes])</div></pre></td></tr></table></figure>
<p><code>none</code> is a placeholder for the batch size. Tensorflow will accept any batch size greater than 0.</p>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batches</span><span class="params">(batch_size, features, labels)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Create batches of features and labels</div><div class="line">    :param batch_size: The batch size</div><div class="line">    :param features: List of features</div><div class="line">    :param labels: List of labels</div><div class="line">    :return: Batches of (Features, Labels)</div><div class="line">    """</div><div class="line">    <span class="keyword">assert</span> len(features) == len(labels)</div><div class="line">    outout_batches = []</div><div class="line">    </div><div class="line">    sample_size = len(features)</div><div class="line">    <span class="keyword">for</span> start_i <span class="keyword">in</span> range(<span class="number">0</span>, sample_size, batch_size):</div><div class="line">        end_i = start_i + batch_size</div><div class="line">        batch = [features[start_i:end_i], 		  				 			 labels[start_i:end_i]]</div><div class="line">        outout_batches.append(batch)</div><div class="line">        </div><div class="line">    <span class="keyword">return</span> outout_batches</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Calculate-Accuracy"><a href="#Calculate-Accuracy" class="headerlink" title="Calculate Accuracy"></a>Calculate Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>), tf.argmax(labels,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h2><p><code>tf.nn.relu()</code></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff3zdtpq0jj30yw0kyt9u.jpg" alt="adding non-linearity"></p>
<h2 id="MNIST-example"><a href="#MNIST-example" class="headerlink" title="MNIST example"></a>MNIST example</h2><p>TF example for MNIST data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Import data</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"."</span>, one_hot=<span class="keyword">True</span>, reshape=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># Set parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_epochs = <span class="number">20</span></div><div class="line">batch_size = <span class="number">128</span>  <span class="comment"># Decrease batch size if you don't have enough memory</span></div><div class="line">display_step = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Model size</span></div><div class="line">n_input = <span class="number">784</span>  <span class="comment"># MNIST data input (img shape: 28*28)</span></div><div class="line">n_classes = <span class="number">10</span>  <span class="comment"># MNIST total classes (0-9 digits)</span></div><div class="line">n_hidden_layer = <span class="number">256</span> <span class="comment"># layer number of features</span></div><div class="line"></div><div class="line"><span class="comment"># Variables: Weights and Bias</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># Input: placeholder</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</div><div class="line">x_flat = tf.reshape(x, [<span class="number">-1</span>, n_input])</div><div class="line"></div><div class="line"><span class="comment"># Computation(linear&amp;non-linear) for hidden layer and output layer</span></div><div class="line"><span class="comment"># Hidden layer with RELU activation</span></div><div class="line">layer_1 = tf.add(tf.matmul(x_flat, weights[<span class="string">'hidden_layer'</span>]),\</div><div class="line">    			 biases[<span class="string">'hidden_layer'</span>])</div><div class="line">layer_1 = tf.nn.relu(layer_1)</div><div class="line"><span class="comment"># Output layer with linear activation</span></div><div class="line">logits = tf.add(tf.matmul(layer_1, weights[<span class="string">'out'</span>]), biases[<span class="string">'out'</span>])</div><div class="line"></div><div class="line"><span class="comment">#Loss and Optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\			  						(logits=logits, labels=y))</div><div class="line">optimizer= tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\</div><div class="line">					.minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    <span class="comment"># Training cycle</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        total_batch = int(mnist.train.num_examples/batch_size)</div><div class="line">        <span class="comment"># Loop over all batches</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</div><div class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</div><div class="line">            <span class="comment"># Run optimization and cost</span></div><div class="line">            sess.run(optimizer, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">        <span class="comment"># Display logs per epoch step</span></div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            c = sess.run(cost, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</div><div class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(c))</div><div class="line">        print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">        </div><div class="line">        <span class="comment"># Test model</span></div><div class="line">        correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    	<span class="comment"># Calculate accuracy</span></div><div class="line">    	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Save-and-Restore-TensorFlow-Models"><a href="#Save-and-Restore-TensorFlow-Models" class="headerlink" title="Save and Restore TensorFlow Models"></a>Save and Restore TensorFlow Models</h2><blockquote>
<p>Training a deep learning model from scratch on a large dataset is expensive computationally. Save the models once finishing training.</p>
</blockquote>
<h3 id="Save-a-Trained-Model"><a href="#Save-a-Trained-Model" class="headerlink" title="Save a Trained Model"></a>Save a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Remove previous Tensors and Operations</span></div><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># start with a model</span></div><div class="line"><span class="comment">## 省略 data, placeholder, Variable, operation, loss and optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># The file path to save the data</span></div><div class="line">save_file = <span class="string">'./train_model.ckpt'</span> <span class="comment"># checkpoint</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># 省略一堆 sess.run() init, optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># Save the model</span></div><div class="line">    saver.save(sess, save_file)</div><div class="line">    print(<span class="string">'Trained Model Saved.'</span>)</div></pre></td></tr></table></figure>
<h3 id="Load-a-Trained-Model"><a href="#Load-a-Trained-Model" class="headerlink" title="Load a Trained Model"></a>Load a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Load the model</span></div><div class="line">    saver.restore(sess, save_file)</div><div class="line">    </div><div class="line">    test_accuracy = sess.run(accuracy,</div><div class="line">        feed_dict=&#123;features: mnist.test.images, labels: mnist.test.labels&#125;)</div><div class="line"></div><div class="line">print(<span class="string">'Test Accuracy: &#123;&#125;'</span>.format(test_accuracy))</div></pre></td></tr></table></figure>
<p>Since <code>tf.train.Saver.restore()</code> sets all the TensorFlow Variables, you don’t need to call <code>tf.global_variables_initializer()</code></p>
<h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h2><p>Loading saved Variables directly into a modified model can generate errors. </p>
<p>具体什么意思呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># suppose we have saved trained model</span></div><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>])) <span class="comment">#name set by TF "Variable:0"</span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]))<span class="comment"># "Variable_1:0"</span></div><div class="line">saver.save(sess, save_file)</div><div class="line"></div><div class="line"><span class="comment"># Remove the previous setting</span></div><div class="line">tf.reset_default_graph()</div><div class="line"><span class="comment"># set new Variables </span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>])) <span class="comment"># "Variable:0"</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]))<span class="comment">#"Variable_1:0"</span></div><div class="line"><span class="comment"># notice that the Variables order changed, the default Variable.name 根据顺序来的</span></div><div class="line">saver.restore(sess, save_file)</div></pre></td></tr></table></figure>
<p><strong>Set the <code>name</code> manually for models.  </strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">'weights'</span>)</div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]), name=<span class="string">'bias'</span>)</div></pre></td></tr></table></figure>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p><code>tf.nn.dropout()</code></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4s30hvc1j30we0fc41h.jpg" alt=""></p>
<p>Dropout after Relu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32) <span class="comment"># probability to keep units</span></div><div class="line">									   </div><div class="line">hidden_layer = tf.add(tf.matmul(features, weights[<span class="number">0</span>]), biases[<span class="number">0</span>])</div><div class="line">hidden_layer = tf.nn.relu(hidden_layer)</div><div class="line">hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)</div></pre></td></tr></table></figure>
<p>During training, a good starting value for <code>keep_prob</code> is 0.5</p>
<p>During testing,  <code>keep_prob = 1</code>  to keep all units and maximize the power of model.</p>
<h2 id="TensorFlow-vs-Numpy"><a href="#TensorFlow-vs-Numpy" class="headerlink" title="TensorFlow vs Numpy"></a>TensorFlow vs Numpy</h2><p>Source from <a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf" target="_blank" rel="external">TensorFlow Tutorial</a></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff4vjzpabrj31fu0qytea.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4vhqxav3j31ge0poafm.jpg" alt=""></p>
<p>TensorFlow computations define a computation graph that has no numerical value until evaluated!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ta = tf.zeros((<span class="number">2</span>,<span class="number">2</span>))</div><div class="line">print(ta.eval())</div></pre></td></tr></table></figure>
<h2 id="TF-Session-Object"><a href="#TF-Session-Object" class="headerlink" title="TF Session Object"></a>TF Session Object</h2></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yuanfresa.github.io/TensorFlow/TensorFlow-学习笔记1/" data-id="cj24orhkd0000nrpnix34fdym" class="article-share-link">Share</a><div class="tags"><a href="/tags/deep-learning/">deep learning</a></div><div class="post-nav"><a href="/paper-notes/关于-Batch-Normalization/" class="pre">关于 Batch Normalization</a><a href="/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/" class="next">CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</a></div><div id="disqus_thread"><script>var disqus_shortname = 'yuanfresa';
var disqus_identifier = 'TensorFlow/TensorFlow-学习笔记1/';
var disqus_title = 'TensorFlow 学习笔记1';
var disqus_url = 'http://yuanfresa.github.io/TensorFlow/TensorFlow-学习笔记1/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//yuanfresa.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yuanfresa.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n-notebook/">CS231n notebook</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper-notes/">paper notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/生命在于折腾/">生命在于折腾</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/TensorFlow/preprocessing-the-data/">preprocessing the data</a></li><li class="post-list-item"><a class="post-list-link" href="/paper-notes/关于-Batch-Normalization/">关于 Batch Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/TensorFlow/TensorFlow-学习笔记1/">TensorFlow 学习笔记1</a></li><li class="post-list-item"><a class="post-list-link" href="/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/">CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/">CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/生命在于折腾/hexo填坑记/">hexo填坑记</a></li><li class="post-list-item"><a class="post-list-link" href="/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-总结笔记/">CS231n Neural Network part1: Setting up the Architecture 总结笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//yuanfresa.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/yuanfresa" title="github" target="_blank">github</a><ul></ul><a href="https://www.zhihu.com/people/yumemor" title="zhihu" target="_blank">zhihu</a><ul></ul><a href="https://twitter.com/Yuan_Fresa/" title="twitter" target="_blank">twitter</a><ul></ul><a href="https://www.facebook.com/yuan.zhou.31392" title="facebook" target="_blank">facebook</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a>2017 </a><a href="/." rel="nofollow">大腿成长记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>å¤§è…¿æˆé•¿è®°</title>
  <subtitle>Keep my dream alive, what if it come true?</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanfresa.github.io/"/>
  <updated>2017-04-30T12:13:24.000Z</updated>
  <id>http://yuanfresa.github.io/</id>
  
  <author>
    <name>Yuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>preprocessing the data</title>
    <link href="http://yuanfresa.github.io/TensorFlow/preprocessing-the-data/"/>
    <id>http://yuanfresa.github.io/TensorFlow/preprocessing-the-data/</id>
    <published>2017-04-30T09:56:26.000Z</published>
    <updated>2017-04-30T12:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>å¹¶ä¸æ˜¯æ‰€æœ‰çš„dataéƒ½ä¼šåƒmnisté‚£æ ·ç›´æ¥å¯ä»¥ç›´æ¥ç”¨</p>
<p><code>from tensorflow.examples.tutorials.mnist import input_data</code> </p>
<p>How can we input external data into TensorFlow</p>
<h2 id="From-csv-to-Tensor"><a href="#From-csv-to-Tensor" class="headerlink" title="From .csv to Tensor"></a>From .csv to Tensor</h2><p>If we have data stored in .csv file and we want to prepare data for the NN like below.</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff4u2j4r3yj31260osmyx.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># A beautiful library to help us work with data as tables</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="comment"># read data</span></div><div class="line">dataframe = pd.read_csv(<span class="string">"data.csv"</span>) </div><div class="line"><span class="comment"># Remove columns we don't care about</span></div><div class="line">dataframe = dataframe.drop([<span class="string">"x3"</span>, <span class="string">"x4"</span>, <span class="string">"x5"</span>], axis = <span class="number">1</span>)</div><div class="line"><span class="comment"># Shape it in matrices to feed it to TensorFlow</span></div><div class="line">inputX = dataframe.loc[:, [<span class="string">'x1'</span>, <span class="string">'x2'</span>]].as_matrix()</div><div class="line">outputY = dataframe.loc[:, [<span class="string">'y1'</span>,<span class="string">'y2'</span>]].as_matrix()</div></pre></td></tr></table></figure>
<h2 id="From-Numpy-to-Tensor"><a href="#From-Numpy-to-Tensor" class="headerlink" title="From Numpy to Tensor"></a>From Numpy to Tensor</h2><ol>
<li><code>tf.convert_to_tensor(a)</code> convenient but doesnâ€™t scale</li>
</ol>
<ol>
<li><p>Placeholders and Feed Dictionaries</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">input = tf.placeholder(tf.float32)</div><div class="line">input_value = ...</div><div class="line">...</div><div class="line">output = computation(input)</div><div class="line">...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(output, feed_dict = &#123;input: input_value&#125;)</div></pre></td></tr></table></figure>
<p>â€‹</p>
<p>â€‹</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      How to prepare data for tensorflow
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yuanfresa.github.io/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>å…³äº Batch Normalization</title>
    <link href="http://yuanfresa.github.io/paper-notes/%E5%85%B3%E4%BA%8E-Batch-Normalization/"/>
    <id>http://yuanfresa.github.io/paper-notes/å…³äº-Batch-Normalization/</id>
    <published>2017-04-28T15:28:14.000Z</published>
    <updated>2017-04-28T15:36:03.000Z</updated>
    
    <summary type="html">
    
      Notes for the paper https://arxiv.org/abs/1502.03167
    
    </summary>
    
      <category term="paper notes" scheme="http://yuanfresa.github.io/categories/paper-notes/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow å­¦ä¹ ç¬”è®°1</title>
    <link href="http://yuanfresa.github.io/TensorFlow/TensorFlow-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
    <id>http://yuanfresa.github.io/TensorFlow/TensorFlow-å­¦ä¹ ç¬”è®°1/</id>
    <published>2017-04-28T15:25:34.000Z</published>
    <updated>2017-04-30T12:41:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>åŸºæœ¬çš„TF functions è¦†ç›–äº†NNçš„åŸºæœ¬çš„æµç¨‹ï¼Œinitialization, activation function, regularization, å¹¶ä»¥ç»å…¸çš„MNIST ä¸ºä¾‹èµ°ä¸€éã€‚å…ˆdefine modelï¼Œä¸å˜çš„æ¯”å¦‚inputï¼Œlabels è®¾ä¸º<code>tf.placeholder()</code>, éœ€è¦updateçš„æ¯”å¦‚weightså’Œbiasè®¾ä¸º<code>tf.Variables()</code>ï¼Œ ç„¶åå®šä¹‰calculationï¼Œæœ€åç”¨<code>tf.Session().run()</code>è®¡ç®—ï¼Œèµ‹å€¼éœ€è¦ç”¨åˆ°<code>feed_dict</code></p>
<h2 id="Basics-of-TensorFlow"><a href="#Basics-of-TensorFlow" class="headerlink" title="Basics of TensorFlow"></a>Basics of TensorFlow</h2><p>Ran operations in<code>tf.Session</code></p>
<p>Create a constant tensor <code>tf.constant()</code></p>
<p>Get input <code>tf.placeholder()</code> ç”¨äºå‡½æ•°runé‡Œå®šä¹‰and <code>feed_dict</code>ç”¨äºtf.Session.run()èµ‹å€¼</p>
<p>å®šä¹‰å˜é‡ <code>tf.Variable()</code></p>
<p>Basic math <code>tf.add(), tf.subtract(), tf.multiply(), tf.divide()</code></p>
<p>Convert data type <code>tf.cast()</code></p>
<h2 id="Linear-Function-in-TF"><a href="#Linear-Function-in-TF" class="headerlink" title="Linear Function in TF"></a>Linear Function in TF</h2><p>Inputs, matrix of the weights and biases</p>
<center>$$ y = xW+b$$</center>

<p>Weights $W$ and bias $b$ need to be a Tensor that can be modified-&gt; <code>tf.Variable</code></p>
<p>Matrix multiplication <code>tf.matmul()</code>, orders matter!</p>
<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>TensorFlow variables must be initialized before they have values.</p>
<p><code>tf.global_variables_initializer()</code> will initialize all TF variables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(init)</div></pre></td></tr></table></figure>
<p>Initialize the random weight from a normal distribution<code>tf.truncated_normal()</code> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">n_features = <span class="number">120</span></div><div class="line">n_labels = <span class="number">5</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([n_features, n_labels]))</div></pre></td></tr></table></figure>
<p>Initialize the bias to all zeros</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bias=tf.Variable(tf.zeros(n_labels))</div></pre></td></tr></table></figure>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>The softmax function squashes itâ€™s inputs, typically called <strong>logits</strong> or <strong>logit scores</strong>, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></div><div class="line">	output = <span class="keyword">None</span></div><div class="line">    logit_data = [<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]</div><div class="line">  	logits = tf.placeholder(tf.float32)</div><div class="line">  	softmax = tf.nn.softmax(logits)</div><div class="line">    </div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        output = sess.run(softmax, feed_dict=&#123;logis: logit_data&#125;)</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff3v4f4isrj30uw0dodgz.jpg" alt="cross entropy loss function"></p>
<p>Sum function<code>tf.reduce_sum()</code> and log function <code>tf.log()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">crossentropy = -tf.reduce_sum(tf.multiply(tf.log(softmax), one_hot))</div></pre></td></tr></table></figure>
<h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><p> Training on subsets of the dataset instead of all the data at one time.</p>
<ol>
<li><p>Divide the data into batches</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Features and Labels</span></div><div class="line">features = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input])</div><div class="line">labels = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes])</div></pre></td></tr></table></figure>
<p><code>none</code> is a placeholder for the batch size. Tensorflow will accept any batch size greater than 0.</p>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batches</span><span class="params">(batch_size, features, labels)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Create batches of features and labels</div><div class="line">    :param batch_size: The batch size</div><div class="line">    :param features: List of features</div><div class="line">    :param labels: List of labels</div><div class="line">    :return: Batches of (Features, Labels)</div><div class="line">    """</div><div class="line">    <span class="keyword">assert</span> len(features) == len(labels)</div><div class="line">    outout_batches = []</div><div class="line">    </div><div class="line">    sample_size = len(features)</div><div class="line">    <span class="keyword">for</span> start_i <span class="keyword">in</span> range(<span class="number">0</span>, sample_size, batch_size):</div><div class="line">        end_i = start_i + batch_size</div><div class="line">        batch = [features[start_i:end_i], 		  				 			 labels[start_i:end_i]]</div><div class="line">        outout_batches.append(batch)</div><div class="line">        </div><div class="line">    <span class="keyword">return</span> outout_batches</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Calculate-Accuracy"><a href="#Calculate-Accuracy" class="headerlink" title="Calculate Accuracy"></a>Calculate Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>), tf.argmax(labels,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h2><p><code>tf.nn.relu()</code></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff3zdtpq0jj30yw0kyt9u.jpg" alt="adding non-linearity"></p>
<h2 id="MNIST-example"><a href="#MNIST-example" class="headerlink" title="MNIST example"></a>MNIST example</h2><p>TF example for MNIST data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Import data</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"."</span>, one_hot=<span class="keyword">True</span>, reshape=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># Set parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_epochs = <span class="number">20</span></div><div class="line">batch_size = <span class="number">128</span>  <span class="comment"># Decrease batch size if you don't have enough memory</span></div><div class="line">display_step = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Model size</span></div><div class="line">n_input = <span class="number">784</span>  <span class="comment"># MNIST data input (img shape: 28*28)</span></div><div class="line">n_classes = <span class="number">10</span>  <span class="comment"># MNIST total classes (0-9 digits)</span></div><div class="line">n_hidden_layer = <span class="number">256</span> <span class="comment"># layer number of features</span></div><div class="line"></div><div class="line"><span class="comment"># Variables: Weights and Bias</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># Input: placeholder</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</div><div class="line">x_flat = tf.reshape(x, [<span class="number">-1</span>, n_input])</div><div class="line"></div><div class="line"><span class="comment"># Computation(linear&amp;non-linear) for hidden layer and output layer</span></div><div class="line"><span class="comment"># Hidden layer with RELU activation</span></div><div class="line">layer_1 = tf.add(tf.matmul(x_flat, weights[<span class="string">'hidden_layer'</span>]),\</div><div class="line">    			 biases[<span class="string">'hidden_layer'</span>])</div><div class="line">layer_1 = tf.nn.relu(layer_1)</div><div class="line"><span class="comment"># Output layer with linear activation</span></div><div class="line">logits = tf.add(tf.matmul(layer_1, weights[<span class="string">'out'</span>]), biases[<span class="string">'out'</span>])</div><div class="line"></div><div class="line"><span class="comment">#Loss and Optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\			  						(logits=logits, labels=y))</div><div class="line">optimizer= tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\</div><div class="line">					.minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    <span class="comment"># Training cycle</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        total_batch = int(mnist.train.num_examples/batch_size)</div><div class="line">        <span class="comment"># Loop over all batches</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</div><div class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</div><div class="line">            <span class="comment"># Run optimization and cost</span></div><div class="line">            sess.run(optimizer, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">        <span class="comment"># Display logs per epoch step</span></div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            c = sess.run(cost, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</div><div class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(c))</div><div class="line">        print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">        </div><div class="line">        <span class="comment"># Test model</span></div><div class="line">        correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    	<span class="comment"># Calculate accuracy</span></div><div class="line">    	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Save-and-Restore-TensorFlow-Models"><a href="#Save-and-Restore-TensorFlow-Models" class="headerlink" title="Save and Restore TensorFlow Models"></a>Save and Restore TensorFlow Models</h2><blockquote>
<p>Training a deep learning model from scratch on a large dataset is expensive computationally. Save the models once finishing training.</p>
</blockquote>
<h3 id="Save-a-Trained-Model"><a href="#Save-a-Trained-Model" class="headerlink" title="Save a Trained Model"></a>Save a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Remove previous Tensors and Operations</span></div><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># start with a model</span></div><div class="line"><span class="comment">## çœç•¥ data, placeholder, Variable, operation, loss and optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># The file path to save the data</span></div><div class="line">save_file = <span class="string">'./train_model.ckpt'</span> <span class="comment"># checkpoint</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># çœç•¥ä¸€å † sess.run() init, optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># Save the model</span></div><div class="line">    saver.save(sess, save_file)</div><div class="line">    print(<span class="string">'Trained Model Saved.'</span>)</div></pre></td></tr></table></figure>
<h3 id="Load-a-Trained-Model"><a href="#Load-a-Trained-Model" class="headerlink" title="Load a Trained Model"></a>Load a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Load the model</span></div><div class="line">    saver.restore(sess, save_file)</div><div class="line">    </div><div class="line">    test_accuracy = sess.run(accuracy,</div><div class="line">        feed_dict=&#123;features: mnist.test.images, labels: mnist.test.labels&#125;)</div><div class="line"></div><div class="line">print(<span class="string">'Test Accuracy: &#123;&#125;'</span>.format(test_accuracy))</div></pre></td></tr></table></figure>
<p>Since <code>tf.train.Saver.restore()</code> sets all the TensorFlow Variables, you donâ€™t need to call <code>tf.global_variables_initializer()</code></p>
<h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h2><p>Loading saved Variables directly into a modified model can generate errors. </p>
<p>å…·ä½“ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># suppose we have saved trained model</span></div><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>])) <span class="comment">#name set by TF "Variable:0"</span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]))<span class="comment"># "Variable_1:0"</span></div><div class="line">saver.save(sess, save_file)</div><div class="line"></div><div class="line"><span class="comment"># Remove the previous setting</span></div><div class="line">tf.reset_default_graph()</div><div class="line"><span class="comment"># set new Variables </span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>])) <span class="comment"># "Variable:0"</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]))<span class="comment">#"Variable_1:0"</span></div><div class="line"><span class="comment"># notice that the Variables order changed, the default Variable.name æ ¹æ®é¡ºåºæ¥çš„</span></div><div class="line">saver.restore(sess, save_file)</div></pre></td></tr></table></figure>
<p><strong>Set the <code>name</code> manually for models.  </strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">'weights'</span>)</div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]), name=<span class="string">'bias'</span>)</div></pre></td></tr></table></figure>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p><code>tf.nn.dropout()</code></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4s30hvc1j30we0fc41h.jpg" alt=""></p>
<p>Dropout after Relu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32) <span class="comment"># probability to keep units</span></div><div class="line">									   </div><div class="line">hidden_layer = tf.add(tf.matmul(features, weights[<span class="number">0</span>]), biases[<span class="number">0</span>])</div><div class="line">hidden_layer = tf.nn.relu(hidden_layer)</div><div class="line">hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)</div></pre></td></tr></table></figure>
<p>During training, a good starting value for <code>keep_prob</code> is 0.5</p>
<p>During testing,  <code>keep_prob = 1</code>  to keep all units and maximize the power of model.</p>
<h2 id="TensorFlow-vs-Numpy"><a href="#TensorFlow-vs-Numpy" class="headerlink" title="TensorFlow vs Numpy"></a>TensorFlow vs Numpy</h2><p>Source from <a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf" target="_blank" rel="external">TensorFlow Tutorial</a></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff4vjzpabrj31fu0qytea.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4vhqxav3j31ge0poafm.jpg" alt=""></p>
<p>TensorFlow computations define a computation graph that has no numerical value until evaluated!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ta = tf.zeros((<span class="number">2</span>,<span class="number">2</span>))</div><div class="line">print(ta.eval())</div></pre></td></tr></table></figure>
<h2 id="TF-Session-Object"><a href="#TF-Session-Object" class="headerlink" title="TF Session Object"></a>TF Session Object</h2>]]></content>
    
    <summary type="html">
    
      TensorFlow notes for the Neural Network
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yuanfresa.github.io/categories/TensorFlow/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part3-Learning-and-Evaluation-æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-26T20:40:51.000Z</published>
    <updated>2017-04-26T20:42:26.000Z</updated>
    
    <summary type="html">
    
      Useful instructions for training
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part2-Setting-up-the-data and the model-æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-26T19:15:35.000Z</published>
    <updated>2017-04-28T15:34:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p><strong>Rules of thumb not laws of the land:</strong> zero-centers and normalizes standard deviations are important for speeding up training</p>
<blockquote>
<p>Computing the mean and subtracting it from every image <em>across the entire dataset</em> and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).</p>
</blockquote>
<p>I think I made a mistake here. Keep it in mind that the mean and the std should be computed only over training data.</p>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>Good methods result in better training.</p>
<h3 id="Weight"><a href="#Weight" class="headerlink" title="Weight"></a>Weight</h3><ul>
<li>HE: Recommended in practice.</li>
</ul>
<ul>
<li>Xavier: Currently use, <font color="Darkorange">todo</font></li>
<li>Derivative of Gaussian: The hidden layers of CNN have an similar effect of computing gradients.</li>
</ul>
<p><a href="Structured Receptive Fields in CNNs">A related paper</a><font color="Darkorange">todo</font></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>We need bias which will increase the flexibility of the model to fit the data.</p>
<p>It is suggested to initialize the biases to be zero.</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><blockquote>
<p>In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. </p>
</blockquote>
<p>After using BN, the speed of convergence increase dramatically. Also it can replace dropout as regularization.<font color="Darkorange">todo</font></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Controlling the capacity of Neural Networks to prevent overfitting.</p>
<p>For regression problem, the loss function, mean squared error(MSE),  works like L2 regularization.</p>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><a href="http://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry/" target="_blank" rel="external">Why L2 loss cause blurry prediction?</a></p>
]]></content>
    
    <summary type="html">
    
      Insights about data preprocessing, initialization, regularization, loss function
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>hexoå¡«å‘è®°</title>
    <link href="http://yuanfresa.github.io/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/hexo%E5%A1%AB%E5%9D%91%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/ç”Ÿå‘½åœ¨äºæŠ˜è…¾/hexoå¡«å‘è®°/</id>
    <published>2017-04-25T10:18:09.000Z</published>
    <updated>2017-04-25T11:59:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo-github-page-æ­å»º"><a href="#hexo-github-page-æ­å»º" class="headerlink" title="hexo+github page æ­å»º"></a>hexo+github page æ­å»º</h2><p>å››æµ·å…«è’çš„å°ä¼™ä¼´ä»¬åˆ†äº«äº†è¶…å¤šç»éªŒï¼ŒğŸ‘‡è¿™ä¸ªå°±å¾ˆæ£’ã€‚</p>
<p><a href="http://www.tianfeifei.com/2016/08/04/tech/hexo%E5%BB%BA%E7%AB%99/" target="_blank" rel="external">è¯¦ç»†æ•™ç¨‹è¯·æ‰¾æˆ‘</a></p>
<h2 id="Themes"><a href="#Themes" class="headerlink" title="Themes"></a>Themes</h2><p>æŒ‘äº†å¥½ä¹…çš„ä¸»é¢˜ï¼Œæœ‰çš„<a href="https://github.com/yumemor/hexo-theme-varaint" target="_blank" rel="external">å¾ˆå¥½çœ‹</a>ä½†æ˜¯ä¸å¤ªé€‚åˆæŠ€æœ¯åšå®¢ï¼Œä¹Ÿæœ‰çš„<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external">å¾ˆç»å…¸</a>ï¼Œ æœ€åæŒ‘äº†ä¸€ä¸ª<a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="external">å¾ˆç®€å•å¾ˆæœ´ç´ çš„</a></p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez4pwx0cjj31760modis.jpg" alt=""></p>
<h2 id="markdown-å·¥å…·åˆ†äº«-for-mac"><a href="#markdown-å·¥å…·åˆ†äº«-for-mac" class="headerlink" title="markdown å·¥å…·åˆ†äº« for mac"></a>markdown å·¥å…·åˆ†äº« for mac</h2><p>ç¥å¥‡çš„markdownã€‚å¾ˆä¹…ä¹‹å‰ç”¨è¿‡Mouï¼Œä½†æ˜¯å‡çº§æˆSierraä¹‹åå°±ä¸èƒ½ç”¨äº†ï¼Œæ”¹ç”¨äº†macdownï¼Œä½†æ˜¯æœ€è¿‘å‘ç°ä¸€ä¸ªæ›´è…»å®³çš„Typoraã€‚</p>
<ul>
<li>macdownæ¯æ¬¡æ¸²æŸ“éƒ½è‡ªåŠ¨è·³åˆ°æ–‡ç« å‰é¢ï¼Œè¿™ç§ç±»ä¼¼æ–‡ç« çš„ä»£ç å’Œç»“æœåˆ†å¼€çœŸçš„æ˜¯å¥½éš¾ç”¨ã€‚ç¥å™¨æ¥äº†ï¼ŒTyporaï¼Œè¿™ä¹ˆæœ´ç´ ç®€æ´ï¼Œå–œæ¬¢çš„ä¸è¦ä¸è¦çš„ã€‚</li>
<li>macdownæ¯æ¬¡æ¢è¡Œè¦ä¸¤æ¬¡spaceä¸€æ¬¡returnï¼ŒçœŸæ˜¯å¤ªéº»çƒ¦äº†</li>
<li>æ”¯æŒmathï¼Œhighlightï¼Œ è™½ç„¶å‘åˆ°github page å¥½åƒæ²¡å•¥ç”¨ï¼Œä½†æ˜¯ç»™todoliståŠ ä¸€ä¸ªHighlightï¼Œä¹Ÿæ˜¯è›®æ¼‚äº®çš„ã€‚<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez4wsjcssj30nw0ko0vx.jpg" alt=""></li>
<li>è¿˜æœ‰æ›´å¤šåŠŸèƒ½æœ‰å¾…æŒ–æ˜ã€‚ã€‚ã€‚==todo==</li>
</ul>
<h2 id="æ’å›¾å°å·¥å…·"><a href="#æ’å›¾å°å·¥å…·" class="headerlink" title="æ’å›¾å°å·¥å…·"></a>æ’å›¾å°å·¥å…·</h2><p>markdownæ’ä¸ªå›¾ç‰‡çœŸçš„è´¹åŠ²</p>
<ul>
<li>qqæˆªä¸ªå›¾å­˜åœ¨æ¡Œé¢</li>
<li>æ‰¾ä¸ªç½‘ç«™ä¸Šä¼ å›¾ç‰‡</li>
<li>å¤åˆ¶å›¾ç‰‡åœ°å€</li>
<li>åœ¨Markdownä¸­ä½¿ç”¨<code>![]()</code>è¯­æ³•è°ƒç”¨å›¾ç‰‡æ’å…¥</li>
</ul>
<p>æ¡Œé¢ä¹±ä¸ƒå…«ç³Ÿä¸å¼€å¿ƒï¼Œè€Œä¸”æ¯æ¬¡éƒ½è¦æ‰¾ä¸ªç½‘ç«™ä¸Šä¼ ï¼Œä½†æ˜¯ç»è¿‡å„ä½çŸ¥ä¹å¤§ç¥çš„ä»‹ç»ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€ä¸ªå¾ˆé…·å¾ˆé…·çš„å°å·¥å…·<a href="https://toolinbox.net/iPic/" target="_blank" rel="external">iPic</a>ï¼Œç°åœ¨æ’å›¾çš„è¯ï¼Œåªéœ€è¦</p>
<ul>
<li>æˆªå›¾</li>
<li>ç‚¹ä¸€ä¸‹å³ä¸Šè§’çš„å°å›¾æ ‡ä¸Šä¼ <img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez564jc1jj301o018glh.jpg" alt=""></li>
<li>è¿˜å¯ä»¥è‡ªåŠ¨ç”Ÿæˆmarkdownæ’å›¾ç‰‡è¯­å¥å“¦</li>
</ul>
<p>ä¸çŸ¥é“ä¼šä¸ä¼šä¸€ç›´å…è´¹ï¼Œå…ˆç”¨ç€å§ã€‚</p>
<h2 id="æ’å…¥æ•°å­¦å…¬å¼"><a href="#æ’å…¥æ•°å­¦å…¬å¼" class="headerlink" title="æ’å…¥æ•°å­¦å…¬å¼"></a>æ’å…¥æ•°å­¦å…¬å¼</h2><p>è™½è¯´Typoraèƒ½å¤Ÿæ¸²æŸ“å‡ºlatexçš„æ•°å­¦å…¬å¼ï¼Œä½†æ˜¯ä¸Šä¼ åˆ°ç½‘é¡µä¸Šå°±ä¸ä¸€å®šæˆåŠŸäº†ï¼Œä¹‹å‰é‚£ä¸ªå¾ˆæ¼‚äº®çš„ä¸»é¢˜å°±æ˜¯ä¸€ç›´å¤±è´¥ï¼Œç»ˆè¢«æˆ‘æŠ›å¼ƒï¼Œ:(</p>
<p>ç½‘ä¸ŠæŸ¥äº†å¾ˆå¤šæ–¹æ³•ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“å“ªä¸ªæœ‰ç”¨ï¼Œå°±éƒ½ç”¨å§ã€‚æˆ‘åšäº†ä»¥ä¸‹è¿™äº›ï¼Œæ˜¯å¯ä»¥æˆåŠŸæ˜¾ç¤ºçš„ã€‚</p>
<h3 id="install-hexo-math"><a href="#install-hexo-math" class="headerlink" title="install hexo-math"></a>install hexo-math</h3><p><code>npm install hexo-math â€”save</code></p>
<h3 id="ä¿®æ”¹-config-yml"><a href="#ä¿®æ”¹-config-yml" class="headerlink" title="ä¿®æ”¹_config.yml"></a>ä¿®æ”¹<code>_config.yml</code></h3><blockquote>
<p>ä¸»ç›®å½•ä¸‹çš„ï¼Œå¦å¤–ä¸»é¢˜ä¸‹ä¹Ÿæœ‰ä¸€ä¸ª_config.ymlï¼Œæ³¨æ„åŒºåˆ†</p>
</blockquote>
<p>åŠ ä¸Š</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># math</div><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<h3 id="æ–‡ç« ä¸­è®¾ç½®"><a href="#æ–‡ç« ä¸­è®¾ç½®" class="headerlink" title="æ–‡ç« ä¸­è®¾ç½®"></a>æ–‡ç« ä¸­è®¾ç½®</h3><p>åœ¨æ–‡ç« éœ€è¦è°ƒç”¨ Mathjax æ—¶, åªéœ€åœ¨ front-matter å‰åŠ ä¸Š <code>mathjax: true</code></p>
<h3 id="latexç”Ÿæˆ"><a href="#latexç”Ÿæˆ" class="headerlink" title="latexç”Ÿæˆ"></a>latexç”Ÿæˆ</h3><p>æœ€åçš„æœ€åç»™å¤§å®¶æ¨èå¥½ç”¨çš„<a href="https://webdemo.myscript.com/views/math.html" target="_blank" rel="external">åœ¨çº¿ç”Ÿæˆlatexå·¥å…·</a>ï¼Œç®€ç›´è¿·å€’äº†ã€‚</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5nzuti2j31kw0p7ag1.jpg" alt=""></p>
<p>æ’å…¥åˆ°æ–‡ç« ä¸­</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span>...<span class="symbol">$</span></div><div class="line"><span class="symbol">$</span><span class="symbol">$</span>...<span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<h2 id="è¯„è®ºå·¥å…·"><a href="#è¯„è®ºå·¥å…·" class="headerlink" title="è¯„è®ºå·¥å…·"></a>è¯„è®ºå·¥å…·</h2><p>ç”»é£æ˜¯è¿™ä¸ªæ ·å­çš„</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5rcdd6qj315g09uq3t.jpg" alt=""></p>
<p>ç®€å•çš„æ¥è¯´ï¼Œé¦–å…ˆå»<a href="https://disqus.com/" target="_blank" rel="external">disqus</a>ç”³è¯·ä¸€ä¸ªè´¦å·ï¼Œå–ä¸€ä¸ªæœ‰è¾¨è¯†åº¦çš„shortnameï¼Œç„¶åé“¾æ¥åˆ°åšå®¢ç½‘ç«™ï¼Œæ­¤å¤„ç¥å‘ï¼Œæ¯”å¦‚æˆ‘æƒ³è¦è¿æ¥åˆ°github pageï¼Œä¸€å®šè¦åŠ ä¸Š<code>http://</code>ï¼Œä¸ç„¶åˆ†äº«çš„æ—¶å€™ä¸€ç›´å‡ºç°<code>yoursite.com</code>ç„¶åä¸åŒçš„ä¸»é¢˜æœ‰ä¸åŒçš„è®¾ç½®ï¼Œæˆ‘ç°åœ¨ç”¨çš„è¿™ä¸ªæç®€ä¸»é¢˜ï¼Œåªéœ€è¦åœ¨_config.ymlä¸­åŠ å…¥</p>
<p><code>disqus: yuanfresa ## Your disqus_shortname, e.g. username</code></p>
<h2 id="RSS-sitemap-è®¾ç½®"><a href="#RSS-sitemap-è®¾ç½®" class="headerlink" title="RSS sitemap è®¾ç½®"></a>RSS sitemap è®¾ç½®</h2><p>ç›®å‰ä¸çŸ¥é“æœ‰ä»€ä¹ˆç”¨ï¼Œçœ‹åˆ°è®¸å¤šæ•™ç¨‹éƒ½ç”¨äº†ï¼Œé‚£å’±ä¹Ÿè¯•è¯•ã€‚ä¾æ—§æ˜¯æ¥è‡ªå››æµ·å…«è’çš„å°ä¼™ä¼´çš„åˆ†äº«ã€‚</p>
<p><a href="http://ijiaober.github.io/2014/08/07/hexo/hexo-08/" target="_blank" rel="external">è¯¦ç»†æ•™ç¨‹è¯·ç‚¹æˆ‘</a></p>
<blockquote>
<p>æ³¨æ„ï¼Œæ­¤å¤„æœ‰å‘ï¼Œsitemapå¥½åƒæ˜¯å¯ä»¥ç”Ÿæˆåˆ†äº«çš„é“¾æ¥ï¼Œæ­¤æ—¶ä¸€å®šè¦åœ¨ä¸»ç›®å½•ä¸‹çš„_config.ymlä¸­ä¿®æ”¹urlï¼Œä¸ç„¶é‚£ä¸ªè®¨åŒçš„<code>yoursite.com</code>åˆä¼šå‡ºç°ã€‚</p>
</blockquote>
<p>å¸Œæœ›å¯ä»¥åšæŒæˆ‘çš„å¤§è…¿è®­ç»ƒï¼Œä¹ˆä¹ˆã€‚</p>
]]></content>
    
    <summary type="html">
    
      é‡åˆ°çš„ä¸€äº›å°å‘ï¼Œè¿˜æœ‰å†™åšå®¢å·¥å…·åˆ†äº«
    
    </summary>
    
      <category term="ç”Ÿå‘½åœ¨äºæŠ˜è…¾" scheme="http://yuanfresa.github.io/categories/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="hexo" scheme="http://yuanfresa.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Neural Network part1: Setting up the Architecture æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-24T20:55:14.000Z</published>
    <updated>2017-04-24T21:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h2><blockquote>
<p>Use the ReLU( $max(x,0)$), be careful with learning rates and possibly monitor the fraction of â€œdeadâ€ units in a network.</p>
</blockquote>
<ol>
<li>Other options</li>
</ol>
<ul>
<li>Leaky ReLU: A small negative slope</li>
</ul>
<p><img src="http://i1.piimg.com/567571/032b87291a258b3f.png" width="300px"></p>
<ul>
<li><p>Maxout: $ max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2}) $</p>
<p>Special case for ReLU($ w_{1}, b_{1}= 0$) and LeakyReLU<br>(+) linear operation, no saturation<br>(+) no dying<br>(-) double the number of parameters for every neuron  </p>
</li>
</ul>
<ol>
<li><p>Learning rate  </p>
<script type="math/tex; mode=display">error = ReLU(x_{n}) - y</script><script type="math/tex; mode=display">\dfrac {\partial error} {\partial x_{n} }=\delta_{n}=\begin{cases} 1, x_{n}\geq 0\\ 0, x_{n}<0 \end{cases}</script><p>The local gradient of ReLU (which is 1) multiply the gradient that flow-back because of back-propagation, the result of the updated gradient could be a large negative number. Proper learning rate to avoid â€œdeadâ€ zero</p>
</li>
<li><p>Monitor the â€œdeadâ€ units<br>How?   <font color="Darkorange">todo</font></p>
</li>
<li><p>Why sigmoid/tanh out of stage?</p>
</li>
</ol>
<ul>
<li>Saturate and kill gradients</li>
<li>Sigmoid outputs are not zero-centered</li>
</ul>
<h2 id="Neural-Network-architectures"><a href="#Neural-Network-architectures" class="headerlink" title="Neural Network architectures"></a>Neural Network architectures</h2><ol>
<li>How many layers?<ul>
<li>Do not count input layer</li>
</ul>
</li>
</ol>
<ul>
<li>Output layer commonly do not have an activation function</li>
</ul>
<ol>
<li><p>How big the network is?</p>
<ul>
<li><p>number of neurons</p>
<p>Itâ€™s easy to count the fully-connected layers, how about convolutional layers?    <font color="Darkorange">todo</font></p>
</li>
<li><p>number of parameters</p>
<p>eg. In keras, use <code>model.summary()</code></p>
</li>
</ul>
</li>
</ol>
<h2 id="Representation-Power"><a href="#Representation-Power" class="headerlink" title="Representation Power"></a>Representation Power</h2><blockquote>
<p>Neural networks are <strong>universal function approximators</strong></p>
</blockquote>
<p>For Neural Networks with fully-connected layers, one hidden layer suffices to approximate any function, whatâ€™s the point of using more layers and going deeper?</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1feygdqnemxj317y0cigoz.jpg" alt=""></p>
<p>However, for CNN, depth has been found to be an extremely importand component for good recognition system.</p>
<ul>
<li>images contain hierarchical structures, so several layers of processing make intuitive sense for this data domain</li>
</ul>
<h2 id="Setting-number-of-layers-and-their-sizes"><a href="#Setting-number-of-layers-and-their-sizes" class="headerlink" title="Setting number of layers and their sizes"></a>Setting number of layers and their sizes</h2><ul>
<li><p>NN with more neurons can express more complicated functions</p>
<ul>
<li>blessing: learn more complicated data</li>
<li>curse: easier to overfit, but it is better to use other method to prevent overfitting instead of reducing the <code>num_neurons</code></li>
</ul>
</li>
<li><p>Regularization strength is the preferred way to control overfitting of a neural network.</p>
<blockquote>
<p>You should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>
</blockquote>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Theoretical bases and practical advices about NN architecture
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>

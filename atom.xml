<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大腿成长记</title>
  <subtitle>Keep my dream alive, what if it come true?</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanfresa.github.io/"/>
  <updated>2017-04-28T15:25:34.000Z</updated>
  <id>http://yuanfresa.github.io/</id>
  
  <author>
    <name>Yuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow 学习笔记</title>
    <link href="http://yuanfresa.github.io/uncategorized/TensorFlow-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/uncategorized/TensorFlow-学习笔记/</id>
    <published>2017-04-28T15:25:34.000Z</published>
    <updated>2017-04-28T15:25:34.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/</id>
    <published>2017-04-26T20:40:51.000Z</published>
    <updated>2017-04-26T20:42:26.000Z</updated>
    
    <summary type="html">
    
      Useful instructions for training
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/</id>
    <published>2017-04-26T19:15:35.000Z</published>
    <updated>2017-04-26T20:21:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p><strong>Rules of thumb not laws of the land:</strong> zero-centers and normalizes standard deviations are important for speeding up training</p>
<blockquote>
<p>Computing the mean and subtracting it from every image <em>across the entire dataset</em> and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).</p>
</blockquote>
<p>I think I made a mistake here. Keep it in mind that the mean and the std should be computed only over training data.</p>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>Good methods result in better training.</p>
<h3 id="Weight"><a href="#Weight" class="headerlink" title="Weight"></a>Weight</h3><ul>
<li>HE: Recommended in practice.</li>
</ul>
<ul>
<li>Xavier: Currently use, <font color="Darkorange">todo</font></li>
<li>Derivative of Gaussian: The hidden layers of CNN have an similar effect of computing gradients.</li>
</ul>
<p><a href="Structured Receptive Fields in CNNs">A related paper</a><font color="Darkorange">todo</font></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>We need bias which will increase the flexibility of the model to fit the data.</p>
<p>It is suggested to initialize the biases to be zero.</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><blockquote>
<p>In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. </p>
</blockquote>
<p>After using BN, the speed of convergence increase dramatically. Also it can replace dropout as regularization.<font color="Darkorange">todo</font></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Controlling the capacity of Neural Networks to prevent overfitting.</p>
<p>For regression problem, the loss function, mean squared error(MSE),  works like L2 regularization.</p>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><a href="http://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry/" target="_blank" rel="external">Why L2 loss cause blurry prediction?</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Data-Preprocessing&quot;&gt;&lt;a href=&quot;#Data-Preprocessing&quot; class=&quot;headerlink&quot; title=&quot;Data Preprocessing&quot;&gt;&lt;/a&gt;Data Preprocessing&lt;/h2&gt;&lt;p&gt;&lt;stron
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>hexo填坑记</title>
    <link href="http://yuanfresa.github.io/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/hexo%E5%A1%AB%E5%9D%91%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/生命在于折腾/hexo填坑记/</id>
    <published>2017-04-25T10:18:09.000Z</published>
    <updated>2017-04-25T11:59:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo-github-page-搭建"><a href="#hexo-github-page-搭建" class="headerlink" title="hexo+github page 搭建"></a>hexo+github page 搭建</h2><p>四海八荒的小伙伴们分享了超多经验，👇这个就很棒。</p>
<p><a href="http://www.tianfeifei.com/2016/08/04/tech/hexo%E5%BB%BA%E7%AB%99/" target="_blank" rel="external">详细教程请找我</a></p>
<h2 id="Themes"><a href="#Themes" class="headerlink" title="Themes"></a>Themes</h2><p>挑了好久的主题，有的<a href="https://github.com/yumemor/hexo-theme-varaint" target="_blank" rel="external">很好看</a>但是不太适合技术博客，也有的<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external">很经典</a>， 最后挑了一个<a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="external">很简单很朴素的</a></p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez4pwx0cjj31760modis.jpg" alt=""></p>
<h2 id="markdown-工具分享-for-mac"><a href="#markdown-工具分享-for-mac" class="headerlink" title="markdown 工具分享 for mac"></a>markdown 工具分享 for mac</h2><p>神奇的markdown。很久之前用过Mou，但是升级成Sierra之后就不能用了，改用了macdown，但是最近发现一个更腻害的Typora。</p>
<ul>
<li>macdown每次渲染都自动跳到文章前面，这种类似文章的代码和结果分开真的是好难用。神器来了，Typora，这么朴素简洁，喜欢的不要不要的。</li>
<li>macdown每次换行要两次space一次return，真是太麻烦了</li>
<li>支持math，highlight， 虽然发到github page 好像没啥用，但是给todolist加一个Highlight，也是蛮漂亮的。<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez4wsjcssj30nw0ko0vx.jpg" alt=""></li>
<li>还有更多功能有待挖掘。。。==todo==</li>
</ul>
<h2 id="插图小工具"><a href="#插图小工具" class="headerlink" title="插图小工具"></a>插图小工具</h2><p>markdown插个图片真的费劲</p>
<ul>
<li>qq截个图存在桌面</li>
<li>找个网站上传图片</li>
<li>复制图片地址</li>
<li>在Markdown中使用<code>![]()</code>语法调用图片插入</li>
</ul>
<p>桌面乱七八糟不开心，而且每次都要找个网站上传，但是经过各位知乎大神的介绍，我找到了一个很酷很酷的小工具<a href="https://toolinbox.net/iPic/" target="_blank" rel="external">iPic</a>，现在插图的话，只需要</p>
<ul>
<li>截图</li>
<li>点一下右上角的小图标上传<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez564jc1jj301o018glh.jpg" alt=""></li>
<li>还可以自动生成markdown插图片语句哦</li>
</ul>
<p>不知道会不会一直免费，先用着吧。</p>
<h2 id="插入数学公式"><a href="#插入数学公式" class="headerlink" title="插入数学公式"></a>插入数学公式</h2><p>虽说Typora能够渲染出latex的数学公式，但是上传到网页上就不一定成功了，之前那个很漂亮的主题就是一直失败，终被我抛弃，:(</p>
<p>网上查了很多方法，我也不知道哪个有用，就都用吧。我做了以下这些，是可以成功显示的。</p>
<h3 id="install-hexo-math"><a href="#install-hexo-math" class="headerlink" title="install hexo-math"></a>install hexo-math</h3><p><code>npm install hexo-math —save</code></p>
<h3 id="修改-config-yml"><a href="#修改-config-yml" class="headerlink" title="修改_config.yml"></a>修改<code>_config.yml</code></h3><blockquote>
<p>主目录下的，另外主题下也有一个_config.yml，注意区分</p>
</blockquote>
<p>加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># math</div><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<h3 id="文章中设置"><a href="#文章中设置" class="headerlink" title="文章中设置"></a>文章中设置</h3><p>在文章需要调用 Mathjax 时, 只需在 front-matter 前加上 <code>mathjax: true</code></p>
<h3 id="latex生成"><a href="#latex生成" class="headerlink" title="latex生成"></a>latex生成</h3><p>最后的最后给大家推荐好用的<a href="https://webdemo.myscript.com/views/math.html" target="_blank" rel="external">在线生成latex工具</a>，简直迷倒了。</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5nzuti2j31kw0p7ag1.jpg" alt=""></p>
<p>插入到文章中</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span>...<span class="symbol">$</span></div><div class="line"><span class="symbol">$</span><span class="symbol">$</span>...<span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<h2 id="评论工具"><a href="#评论工具" class="headerlink" title="评论工具"></a>评论工具</h2><p>画风是这个样子的</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5rcdd6qj315g09uq3t.jpg" alt=""></p>
<p>简单的来说，首先去<a href="https://disqus.com/" target="_blank" rel="external">disqus</a>申请一个账号，取一个有辨识度的shortname，然后链接到博客网站，此处神坑，比如我想要连接到github page，一定要加上<code>http://</code>，不然分享的时候一直出现<code>yoursite.com</code>然后不同的主题有不同的设置，我现在用的这个极简主题，只需要在_config.yml中加入</p>
<p><code>disqus: yuanfresa ## Your disqus_shortname, e.g. username</code></p>
<h2 id="RSS-sitemap-设置"><a href="#RSS-sitemap-设置" class="headerlink" title="RSS sitemap 设置"></a>RSS sitemap 设置</h2><p>目前不知道有什么用，看到许多教程都用了，那咱也试试。依旧是来自四海八荒的小伙伴的分享。</p>
<p><a href="http://ijiaober.github.io/2014/08/07/hexo/hexo-08/" target="_blank" rel="external">详细教程请点我</a></p>
<blockquote>
<p>注意，此处有坑，sitemap好像是可以生成分享的链接，此时一定要在主目录下的_config.yml中修改url，不然那个讨厌的<code>yoursite.com</code>又会出现。</p>
</blockquote>
<p>希望可以坚持我的大腿训练，么么。</p>
]]></content>
    
    <summary type="html">
    
      遇到的一些小坑，还有写博客工具分享
    
    </summary>
    
      <category term="生命在于折腾" scheme="http://yuanfresa.github.io/categories/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="hexo" scheme="http://yuanfresa.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Neural Network part1: Setting up the Architecture 总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-总结笔记/</id>
    <published>2017-04-24T20:55:14.000Z</published>
    <updated>2017-04-24T21:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h2><blockquote>
<p>Use the ReLU( $max(x,0)$), be careful with learning rates and possibly monitor the fraction of “dead” units in a network.</p>
</blockquote>
<ol>
<li>Other options</li>
</ol>
<ul>
<li>Leaky ReLU: A small negative slope</li>
</ul>
<p><img src="http://i1.piimg.com/567571/032b87291a258b3f.png" width="300px"></p>
<ul>
<li><p>Maxout: $ max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2}) $</p>
<p>Special case for ReLU($ w_{1}, b_{1}= 0$) and LeakyReLU<br>(+) linear operation, no saturation<br>(+) no dying<br>(-) double the number of parameters for every neuron  </p>
</li>
</ul>
<ol>
<li><p>Learning rate  </p>
<script type="math/tex; mode=display">error = ReLU(x_{n}) - y</script><script type="math/tex; mode=display">\dfrac {\partial error} {\partial x_{n} }=\delta_{n}=\begin{cases} 1, x_{n}\geq 0\\ 0, x_{n}<0 \end{cases}</script><p>The local gradient of ReLU (which is 1) multiply the gradient that flow-back because of back-propagation, the result of the updated gradient could be a large negative number. Proper learning rate to avoid “dead” zero</p>
</li>
<li><p>Monitor the “dead” units<br>How?   <font color="Darkorange">todo</font></p>
</li>
<li><p>Why sigmoid/tanh out of stage?</p>
</li>
</ol>
<ul>
<li>Saturate and kill gradients</li>
<li>Sigmoid outputs are not zero-centered</li>
</ul>
<h2 id="Neural-Network-architectures"><a href="#Neural-Network-architectures" class="headerlink" title="Neural Network architectures"></a>Neural Network architectures</h2><ol>
<li>How many layers?<ul>
<li>Do not count input layer</li>
</ul>
</li>
</ol>
<ul>
<li>Output layer commonly do not have an activation function</li>
</ul>
<ol>
<li><p>How big the network is?</p>
<ul>
<li><p>number of neurons</p>
<p>It’s easy to count the fully-connected layers, how about convolutional layers?    <font color="Darkorange">todo</font></p>
</li>
<li><p>number of parameters</p>
<p>eg. In keras, use <code>model.summary()</code></p>
</li>
</ul>
</li>
</ol>
<h2 id="Representation-Power"><a href="#Representation-Power" class="headerlink" title="Representation Power"></a>Representation Power</h2><blockquote>
<p>Neural networks are <strong>universal function approximators</strong></p>
</blockquote>
<p>For Neural Networks with fully-connected layers, one hidden layer suffices to approximate any function, what’s the point of using more layers and going deeper?</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1feygdqnemxj317y0cigoz.jpg" alt=""></p>
<p>However, for CNN, depth has been found to be an extremely importand component for good recognition system.</p>
<ul>
<li>images contain hierarchical structures, so several layers of processing make intuitive sense for this data domain</li>
</ul>
<h2 id="Setting-number-of-layers-and-their-sizes"><a href="#Setting-number-of-layers-and-their-sizes" class="headerlink" title="Setting number of layers and their sizes"></a>Setting number of layers and their sizes</h2><ul>
<li><p>NN with more neurons can express more complicated functions</p>
<ul>
<li>blessing: learn more complicated data</li>
<li>curse: easier to overfit, but it is better to use other method to prevent overfitting instead of reducing the <code>num_neurons</code></li>
</ul>
</li>
<li><p>Regularization strength is the preferred way to control overfitting of a neural network.</p>
<blockquote>
<p>You should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>
</blockquote>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Theoretical bases and practical advices about NN architecture
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>大腿成长记</title>
  <subtitle>Keep my dream alive, what if it come true?</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanfresa.github.io/"/>
  <updated>2017-04-30T12:13:24.000Z</updated>
  <id>http://yuanfresa.github.io/</id>
  
  <author>
    <name>Yuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>preprocessing the data</title>
    <link href="http://yuanfresa.github.io/TensorFlow/preprocessing-the-data/"/>
    <id>http://yuanfresa.github.io/TensorFlow/preprocessing-the-data/</id>
    <published>2017-04-30T09:56:26.000Z</published>
    <updated>2017-04-30T12:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>并不是所有的data都会像mnist那样直接可以直接用</p>
<p><code>from tensorflow.examples.tutorials.mnist import input_data</code> </p>
<p>How can we input external data into TensorFlow</p>
<h2 id="From-csv-to-Tensor"><a href="#From-csv-to-Tensor" class="headerlink" title="From .csv to Tensor"></a>From .csv to Tensor</h2><p>If we have data stored in .csv file and we want to prepare data for the NN like below.</p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff4u2j4r3yj31260osmyx.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># A beautiful library to help us work with data as tables</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="comment"># read data</span></div><div class="line">dataframe = pd.read_csv(<span class="string">"data.csv"</span>) </div><div class="line"><span class="comment"># Remove columns we don't care about</span></div><div class="line">dataframe = dataframe.drop([<span class="string">"x3"</span>, <span class="string">"x4"</span>, <span class="string">"x5"</span>], axis = <span class="number">1</span>)</div><div class="line"><span class="comment"># Shape it in matrices to feed it to TensorFlow</span></div><div class="line">inputX = dataframe.loc[:, [<span class="string">'x1'</span>, <span class="string">'x2'</span>]].as_matrix()</div><div class="line">outputY = dataframe.loc[:, [<span class="string">'y1'</span>,<span class="string">'y2'</span>]].as_matrix()</div></pre></td></tr></table></figure>
<h2 id="From-Numpy-to-Tensor"><a href="#From-Numpy-to-Tensor" class="headerlink" title="From Numpy to Tensor"></a>From Numpy to Tensor</h2><ol>
<li><code>tf.convert_to_tensor(a)</code> convenient but doesn’t scale</li>
</ol>
<ol>
<li><p>Placeholders and Feed Dictionaries</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">input = tf.placeholder(tf.float32)</div><div class="line">input_value = ...</div><div class="line">...</div><div class="line">output = computation(input)</div><div class="line">...</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(output, feed_dict = &#123;input: input_value&#125;)</div></pre></td></tr></table></figure>
<p>​</p>
<p>​</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      How to prepare data for tensorflow
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yuanfresa.github.io/categories/TensorFlow/"/>
    
    
  </entry>
  
  <entry>
    <title>关于 Batch Normalization</title>
    <link href="http://yuanfresa.github.io/paper-notes/%E5%85%B3%E4%BA%8E-Batch-Normalization/"/>
    <id>http://yuanfresa.github.io/paper-notes/关于-Batch-Normalization/</id>
    <published>2017-04-28T15:28:14.000Z</published>
    <updated>2017-04-28T15:36:03.000Z</updated>
    
    <summary type="html">
    
      Notes for the paper https://arxiv.org/abs/1502.03167
    
    </summary>
    
      <category term="paper notes" scheme="http://yuanfresa.github.io/categories/paper-notes/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 学习笔记1</title>
    <link href="http://yuanfresa.github.io/TensorFlow/TensorFlow-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
    <id>http://yuanfresa.github.io/TensorFlow/TensorFlow-学习笔记1/</id>
    <published>2017-04-28T15:25:34.000Z</published>
    <updated>2017-04-30T12:41:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>基本的TF functions 覆盖了NN的基本的流程，initialization, activation function, regularization, 并以经典的MNIST 为例走一遍。先define model，不变的比如input，labels 设为<code>tf.placeholder()</code>, 需要update的比如weights和bias设为<code>tf.Variables()</code>， 然后定义calculation，最后用<code>tf.Session().run()</code>计算，赋值需要用到<code>feed_dict</code></p>
<h2 id="Basics-of-TensorFlow"><a href="#Basics-of-TensorFlow" class="headerlink" title="Basics of TensorFlow"></a>Basics of TensorFlow</h2><p>Ran operations in<code>tf.Session</code></p>
<p>Create a constant tensor <code>tf.constant()</code></p>
<p>Get input <code>tf.placeholder()</code> 用于函数run里定义and <code>feed_dict</code>用于tf.Session.run()赋值</p>
<p>定义变量 <code>tf.Variable()</code></p>
<p>Basic math <code>tf.add(), tf.subtract(), tf.multiply(), tf.divide()</code></p>
<p>Convert data type <code>tf.cast()</code></p>
<h2 id="Linear-Function-in-TF"><a href="#Linear-Function-in-TF" class="headerlink" title="Linear Function in TF"></a>Linear Function in TF</h2><p>Inputs, matrix of the weights and biases</p>
<center>$$ y = xW+b$$</center>

<p>Weights $W$ and bias $b$ need to be a Tensor that can be modified-&gt; <code>tf.Variable</code></p>
<p>Matrix multiplication <code>tf.matmul()</code>, orders matter!</p>
<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>TensorFlow variables must be initialized before they have values.</p>
<p><code>tf.global_variables_initializer()</code> will initialize all TF variables.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">init = tf.global_variables_initializer()</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">	sess.run(init)</div></pre></td></tr></table></figure>
<p>Initialize the random weight from a normal distribution<code>tf.truncated_normal()</code> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">n_features = <span class="number">120</span></div><div class="line">n_labels = <span class="number">5</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([n_features, n_labels]))</div></pre></td></tr></table></figure>
<p>Initialize the bias to all zeros</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bias=tf.Variable(tf.zeros(n_labels))</div></pre></td></tr></table></figure>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>The softmax function squashes it’s inputs, typically called <strong>logits</strong> or <strong>logit scores</strong>, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></div><div class="line">	output = <span class="keyword">None</span></div><div class="line">    logit_data = [<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>]</div><div class="line">  	logits = tf.placeholder(tf.float32)</div><div class="line">  	softmax = tf.nn.softmax(logits)</div><div class="line">    </div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        output = sess.run(softmax, feed_dict=&#123;logis: logit_data&#125;)</div><div class="line">    <span class="keyword">return</span> output</div></pre></td></tr></table></figure>
<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff3v4f4isrj30uw0dodgz.jpg" alt="cross entropy loss function"></p>
<p>Sum function<code>tf.reduce_sum()</code> and log function <code>tf.log()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">crossentropy = -tf.reduce_sum(tf.multiply(tf.log(softmax), one_hot))</div></pre></td></tr></table></figure>
<h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><p> Training on subsets of the dataset instead of all the data at one time.</p>
<ol>
<li><p>Divide the data into batches</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Features and Labels</span></div><div class="line">features = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input])</div><div class="line">labels = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes])</div></pre></td></tr></table></figure>
<p><code>none</code> is a placeholder for the batch size. Tensorflow will accept any batch size greater than 0.</p>
</li>
<li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batches</span><span class="params">(batch_size, features, labels)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Create batches of features and labels</div><div class="line">    :param batch_size: The batch size</div><div class="line">    :param features: List of features</div><div class="line">    :param labels: List of labels</div><div class="line">    :return: Batches of (Features, Labels)</div><div class="line">    """</div><div class="line">    <span class="keyword">assert</span> len(features) == len(labels)</div><div class="line">    outout_batches = []</div><div class="line">    </div><div class="line">    sample_size = len(features)</div><div class="line">    <span class="keyword">for</span> start_i <span class="keyword">in</span> range(<span class="number">0</span>, sample_size, batch_size):</div><div class="line">        end_i = start_i + batch_size</div><div class="line">        batch = [features[start_i:end_i], 		  				 			 labels[start_i:end_i]]</div><div class="line">        outout_batches.append(batch)</div><div class="line">        </div><div class="line">    <span class="keyword">return</span> outout_batches</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Calculate-Accuracy"><a href="#Calculate-Accuracy" class="headerlink" title="Calculate Accuracy"></a>Calculate Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>), tf.argmax(labels,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h2><p><code>tf.nn.relu()</code></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff3zdtpq0jj30yw0kyt9u.jpg" alt="adding non-linearity"></p>
<h2 id="MNIST-example"><a href="#MNIST-example" class="headerlink" title="MNIST example"></a>MNIST example</h2><p>TF example for MNIST data</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Import data</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"."</span>, one_hot=<span class="keyword">True</span>, reshape=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># Set parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_epochs = <span class="number">20</span></div><div class="line">batch_size = <span class="number">128</span>  <span class="comment"># Decrease batch size if you don't have enough memory</span></div><div class="line">display_step = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Model size</span></div><div class="line">n_input = <span class="number">784</span>  <span class="comment"># MNIST data input (img shape: 28*28)</span></div><div class="line">n_classes = <span class="number">10</span>  <span class="comment"># MNIST total classes (0-9 digits)</span></div><div class="line">n_hidden_layer = <span class="number">256</span> <span class="comment"># layer number of features</span></div><div class="line"></div><div class="line"><span class="comment"># Variables: Weights and Bias</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'hidden_layer'</span>: tf.Variable(tf.random_normal([n_hidden_layer])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># Input: placeholder</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</div><div class="line">x_flat = tf.reshape(x, [<span class="number">-1</span>, n_input])</div><div class="line"></div><div class="line"><span class="comment"># Computation(linear&amp;non-linear) for hidden layer and output layer</span></div><div class="line"><span class="comment"># Hidden layer with RELU activation</span></div><div class="line">layer_1 = tf.add(tf.matmul(x_flat, weights[<span class="string">'hidden_layer'</span>]),\</div><div class="line">    			 biases[<span class="string">'hidden_layer'</span>])</div><div class="line">layer_1 = tf.nn.relu(layer_1)</div><div class="line"><span class="comment"># Output layer with linear activation</span></div><div class="line">logits = tf.add(tf.matmul(layer_1, weights[<span class="string">'out'</span>]), biases[<span class="string">'out'</span>])</div><div class="line"></div><div class="line"><span class="comment">#Loss and Optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\			  						(logits=logits, labels=y))</div><div class="line">optimizer= tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\</div><div class="line">					.minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line">    <span class="comment"># Training cycle</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        total_batch = int(mnist.train.num_examples/batch_size)</div><div class="line">        <span class="comment"># Loop over all batches</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</div><div class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</div><div class="line">            <span class="comment"># Run optimization and cost</span></div><div class="line">            sess.run(optimizer, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">        <span class="comment"># Display logs per epoch step</span></div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            c = sess.run(cost, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</div><div class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(c))</div><div class="line">        print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">        </div><div class="line">        <span class="comment"># Test model</span></div><div class="line">        correct_prediction = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    	<span class="comment"># Calculate accuracy</span></div><div class="line">    	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure>
<h2 id="Save-and-Restore-TensorFlow-Models"><a href="#Save-and-Restore-TensorFlow-Models" class="headerlink" title="Save and Restore TensorFlow Models"></a>Save and Restore TensorFlow Models</h2><blockquote>
<p>Training a deep learning model from scratch on a large dataset is expensive computationally. Save the models once finishing training.</p>
</blockquote>
<h3 id="Save-a-Trained-Model"><a href="#Save-a-Trained-Model" class="headerlink" title="Save a Trained Model"></a>Save a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Remove previous Tensors and Operations</span></div><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># start with a model</span></div><div class="line"><span class="comment">## 省略 data, placeholder, Variable, operation, loss and optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># The file path to save the data</span></div><div class="line">save_file = <span class="string">'./train_model.ckpt'</span> <span class="comment"># checkpoint</span></div><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># 省略一堆 sess.run() init, optimizer, accuracy</span></div><div class="line"></div><div class="line"><span class="comment"># Save the model</span></div><div class="line">    saver.save(sess, save_file)</div><div class="line">    print(<span class="string">'Trained Model Saved.'</span>)</div></pre></td></tr></table></figure>
<h3 id="Load-a-Trained-Model"><a href="#Load-a-Trained-Model" class="headerlink" title="Load a Trained Model"></a>Load a Trained Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">saver = tf.train.Saver()</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Load the model</span></div><div class="line">    saver.restore(sess, save_file)</div><div class="line">    </div><div class="line">    test_accuracy = sess.run(accuracy,</div><div class="line">        feed_dict=&#123;features: mnist.test.images, labels: mnist.test.labels&#125;)</div><div class="line"></div><div class="line">print(<span class="string">'Test Accuracy: &#123;&#125;'</span>.format(test_accuracy))</div></pre></td></tr></table></figure>
<p>Since <code>tf.train.Saver.restore()</code> sets all the TensorFlow Variables, you don’t need to call <code>tf.global_variables_initializer()</code></p>
<h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a>Fine Tuning</h2><p>Loading saved Variables directly into a modified model can generate errors. </p>
<p>具体什么意思呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># suppose we have saved trained model</span></div><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>])) <span class="comment">#name set by TF "Variable:0"</span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]))<span class="comment"># "Variable_1:0"</span></div><div class="line">saver.save(sess, save_file)</div><div class="line"></div><div class="line"><span class="comment"># Remove the previous setting</span></div><div class="line">tf.reset_default_graph()</div><div class="line"><span class="comment"># set new Variables </span></div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>])) <span class="comment"># "Variable:0"</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]))<span class="comment">#"Variable_1:0"</span></div><div class="line"><span class="comment"># notice that the Variables order changed, the default Variable.name 根据顺序来的</span></div><div class="line">saver.restore(sess, save_file)</div></pre></td></tr></table></figure>
<p><strong>Set the <code>name</code> manually for models.  </strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Two Tensor Variables: weights and bias</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">'weights'</span>)</div><div class="line">bias = tf.Variable(tf.truncated_normal([<span class="number">3</span>]), name=<span class="string">'bias'</span>)</div></pre></td></tr></table></figure>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p><code>tf.nn.dropout()</code></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4s30hvc1j30we0fc41h.jpg" alt=""></p>
<p>Dropout after Relu</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">keep_prob = tf.placeholder(tf.float32) <span class="comment"># probability to keep units</span></div><div class="line">									   </div><div class="line">hidden_layer = tf.add(tf.matmul(features, weights[<span class="number">0</span>]), biases[<span class="number">0</span>])</div><div class="line">hidden_layer = tf.nn.relu(hidden_layer)</div><div class="line">hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)</div></pre></td></tr></table></figure>
<p>During training, a good starting value for <code>keep_prob</code> is 0.5</p>
<p>During testing,  <code>keep_prob = 1</code>  to keep all units and maximize the power of model.</p>
<h2 id="TensorFlow-vs-Numpy"><a href="#TensorFlow-vs-Numpy" class="headerlink" title="TensorFlow vs Numpy"></a>TensorFlow vs Numpy</h2><p>Source from <a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf" target="_blank" rel="external">TensorFlow Tutorial</a></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNbRwgy1ff4vjzpabrj31fu0qytea.jpg" alt=""></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNbRwgy1ff4vhqxav3j31ge0poafm.jpg" alt=""></p>
<p>TensorFlow computations define a computation graph that has no numerical value until evaluated!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ta = tf.zeros((<span class="number">2</span>,<span class="number">2</span>))</div><div class="line">print(ta.eval())</div></pre></td></tr></table></figure>
<h2 id="TF-Session-Object"><a href="#TF-Session-Object" class="headerlink" title="TF Session Object"></a>TF Session Object</h2>]]></content>
    
    <summary type="html">
    
      TensorFlow notes for the Neural Network
    
    </summary>
    
      <category term="TensorFlow" scheme="http://yuanfresa.github.io/categories/TensorFlow/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/</id>
    <published>2017-04-26T20:40:51.000Z</published>
    <updated>2017-04-26T20:42:26.000Z</updated>
    
    <summary type="html">
    
      Useful instructions for training
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/</id>
    <published>2017-04-26T19:15:35.000Z</published>
    <updated>2017-04-28T15:34:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p><strong>Rules of thumb not laws of the land:</strong> zero-centers and normalizes standard deviations are important for speeding up training</p>
<blockquote>
<p>Computing the mean and subtracting it from every image <em>across the entire dataset</em> and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).</p>
</blockquote>
<p>I think I made a mistake here. Keep it in mind that the mean and the std should be computed only over training data.</p>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>Good methods result in better training.</p>
<h3 id="Weight"><a href="#Weight" class="headerlink" title="Weight"></a>Weight</h3><ul>
<li>HE: Recommended in practice.</li>
</ul>
<ul>
<li>Xavier: Currently use, <font color="Darkorange">todo</font></li>
<li>Derivative of Gaussian: The hidden layers of CNN have an similar effect of computing gradients.</li>
</ul>
<p><a href="Structured Receptive Fields in CNNs">A related paper</a><font color="Darkorange">todo</font></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>We need bias which will increase the flexibility of the model to fit the data.</p>
<p>It is suggested to initialize the biases to be zero.</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><blockquote>
<p>In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. </p>
</blockquote>
<p>After using BN, the speed of convergence increase dramatically. Also it can replace dropout as regularization.<font color="Darkorange">todo</font></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Controlling the capacity of Neural Networks to prevent overfitting.</p>
<p>For regression problem, the loss function, mean squared error(MSE),  works like L2 regularization.</p>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><a href="http://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry/" target="_blank" rel="external">Why L2 loss cause blurry prediction?</a></p>
]]></content>
    
    <summary type="html">
    
      Insights about data preprocessing, initialization, regularization, loss function
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>hexo填坑记</title>
    <link href="http://yuanfresa.github.io/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/hexo%E5%A1%AB%E5%9D%91%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/生命在于折腾/hexo填坑记/</id>
    <published>2017-04-25T10:18:09.000Z</published>
    <updated>2017-04-25T11:59:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo-github-page-搭建"><a href="#hexo-github-page-搭建" class="headerlink" title="hexo+github page 搭建"></a>hexo+github page 搭建</h2><p>四海八荒的小伙伴们分享了超多经验，👇这个就很棒。</p>
<p><a href="http://www.tianfeifei.com/2016/08/04/tech/hexo%E5%BB%BA%E7%AB%99/" target="_blank" rel="external">详细教程请找我</a></p>
<h2 id="Themes"><a href="#Themes" class="headerlink" title="Themes"></a>Themes</h2><p>挑了好久的主题，有的<a href="https://github.com/yumemor/hexo-theme-varaint" target="_blank" rel="external">很好看</a>但是不太适合技术博客，也有的<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external">很经典</a>， 最后挑了一个<a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="external">很简单很朴素的</a></p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez4pwx0cjj31760modis.jpg" alt=""></p>
<h2 id="markdown-工具分享-for-mac"><a href="#markdown-工具分享-for-mac" class="headerlink" title="markdown 工具分享 for mac"></a>markdown 工具分享 for mac</h2><p>神奇的markdown。很久之前用过Mou，但是升级成Sierra之后就不能用了，改用了macdown，但是最近发现一个更腻害的Typora。</p>
<ul>
<li>macdown每次渲染都自动跳到文章前面，这种类似文章的代码和结果分开真的是好难用。神器来了，Typora，这么朴素简洁，喜欢的不要不要的。</li>
<li>macdown每次换行要两次space一次return，真是太麻烦了</li>
<li>支持math，highlight， 虽然发到github page 好像没啥用，但是给todolist加一个Highlight，也是蛮漂亮的。<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez4wsjcssj30nw0ko0vx.jpg" alt=""></li>
<li>还有更多功能有待挖掘。。。==todo==</li>
</ul>
<h2 id="插图小工具"><a href="#插图小工具" class="headerlink" title="插图小工具"></a>插图小工具</h2><p>markdown插个图片真的费劲</p>
<ul>
<li>qq截个图存在桌面</li>
<li>找个网站上传图片</li>
<li>复制图片地址</li>
<li>在Markdown中使用<code>![]()</code>语法调用图片插入</li>
</ul>
<p>桌面乱七八糟不开心，而且每次都要找个网站上传，但是经过各位知乎大神的介绍，我找到了一个很酷很酷的小工具<a href="https://toolinbox.net/iPic/" target="_blank" rel="external">iPic</a>，现在插图的话，只需要</p>
<ul>
<li>截图</li>
<li>点一下右上角的小图标上传<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez564jc1jj301o018glh.jpg" alt=""></li>
<li>还可以自动生成markdown插图片语句哦</li>
</ul>
<p>不知道会不会一直免费，先用着吧。</p>
<h2 id="插入数学公式"><a href="#插入数学公式" class="headerlink" title="插入数学公式"></a>插入数学公式</h2><p>虽说Typora能够渲染出latex的数学公式，但是上传到网页上就不一定成功了，之前那个很漂亮的主题就是一直失败，终被我抛弃，:(</p>
<p>网上查了很多方法，我也不知道哪个有用，就都用吧。我做了以下这些，是可以成功显示的。</p>
<h3 id="install-hexo-math"><a href="#install-hexo-math" class="headerlink" title="install hexo-math"></a>install hexo-math</h3><p><code>npm install hexo-math —save</code></p>
<h3 id="修改-config-yml"><a href="#修改-config-yml" class="headerlink" title="修改_config.yml"></a>修改<code>_config.yml</code></h3><blockquote>
<p>主目录下的，另外主题下也有一个_config.yml，注意区分</p>
</blockquote>
<p>加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># math</div><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<h3 id="文章中设置"><a href="#文章中设置" class="headerlink" title="文章中设置"></a>文章中设置</h3><p>在文章需要调用 Mathjax 时, 只需在 front-matter 前加上 <code>mathjax: true</code></p>
<h3 id="latex生成"><a href="#latex生成" class="headerlink" title="latex生成"></a>latex生成</h3><p>最后的最后给大家推荐好用的<a href="https://webdemo.myscript.com/views/math.html" target="_blank" rel="external">在线生成latex工具</a>，简直迷倒了。</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5nzuti2j31kw0p7ag1.jpg" alt=""></p>
<p>插入到文章中</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span>...<span class="symbol">$</span></div><div class="line"><span class="symbol">$</span><span class="symbol">$</span>...<span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<h2 id="评论工具"><a href="#评论工具" class="headerlink" title="评论工具"></a>评论工具</h2><p>画风是这个样子的</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5rcdd6qj315g09uq3t.jpg" alt=""></p>
<p>简单的来说，首先去<a href="https://disqus.com/" target="_blank" rel="external">disqus</a>申请一个账号，取一个有辨识度的shortname，然后链接到博客网站，此处神坑，比如我想要连接到github page，一定要加上<code>http://</code>，不然分享的时候一直出现<code>yoursite.com</code>然后不同的主题有不同的设置，我现在用的这个极简主题，只需要在_config.yml中加入</p>
<p><code>disqus: yuanfresa ## Your disqus_shortname, e.g. username</code></p>
<h2 id="RSS-sitemap-设置"><a href="#RSS-sitemap-设置" class="headerlink" title="RSS sitemap 设置"></a>RSS sitemap 设置</h2><p>目前不知道有什么用，看到许多教程都用了，那咱也试试。依旧是来自四海八荒的小伙伴的分享。</p>
<p><a href="http://ijiaober.github.io/2014/08/07/hexo/hexo-08/" target="_blank" rel="external">详细教程请点我</a></p>
<blockquote>
<p>注意，此处有坑，sitemap好像是可以生成分享的链接，此时一定要在主目录下的_config.yml中修改url，不然那个讨厌的<code>yoursite.com</code>又会出现。</p>
</blockquote>
<p>希望可以坚持我的大腿训练，么么。</p>
]]></content>
    
    <summary type="html">
    
      遇到的一些小坑，还有写博客工具分享
    
    </summary>
    
      <category term="生命在于折腾" scheme="http://yuanfresa.github.io/categories/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="hexo" scheme="http://yuanfresa.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Neural Network part1: Setting up the Architecture 总结笔记</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-总结笔记/</id>
    <published>2017-04-24T20:55:14.000Z</published>
    <updated>2017-04-24T21:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h2><blockquote>
<p>Use the ReLU( $max(x,0)$), be careful with learning rates and possibly monitor the fraction of “dead” units in a network.</p>
</blockquote>
<ol>
<li>Other options</li>
</ol>
<ul>
<li>Leaky ReLU: A small negative slope</li>
</ul>
<p><img src="http://i1.piimg.com/567571/032b87291a258b3f.png" width="300px"></p>
<ul>
<li><p>Maxout: $ max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2}) $</p>
<p>Special case for ReLU($ w_{1}, b_{1}= 0$) and LeakyReLU<br>(+) linear operation, no saturation<br>(+) no dying<br>(-) double the number of parameters for every neuron  </p>
</li>
</ul>
<ol>
<li><p>Learning rate  </p>
<script type="math/tex; mode=display">error = ReLU(x_{n}) - y</script><script type="math/tex; mode=display">\dfrac {\partial error} {\partial x_{n} }=\delta_{n}=\begin{cases} 1, x_{n}\geq 0\\ 0, x_{n}<0 \end{cases}</script><p>The local gradient of ReLU (which is 1) multiply the gradient that flow-back because of back-propagation, the result of the updated gradient could be a large negative number. Proper learning rate to avoid “dead” zero</p>
</li>
<li><p>Monitor the “dead” units<br>How?   <font color="Darkorange">todo</font></p>
</li>
<li><p>Why sigmoid/tanh out of stage?</p>
</li>
</ol>
<ul>
<li>Saturate and kill gradients</li>
<li>Sigmoid outputs are not zero-centered</li>
</ul>
<h2 id="Neural-Network-architectures"><a href="#Neural-Network-architectures" class="headerlink" title="Neural Network architectures"></a>Neural Network architectures</h2><ol>
<li>How many layers?<ul>
<li>Do not count input layer</li>
</ul>
</li>
</ol>
<ul>
<li>Output layer commonly do not have an activation function</li>
</ul>
<ol>
<li><p>How big the network is?</p>
<ul>
<li><p>number of neurons</p>
<p>It’s easy to count the fully-connected layers, how about convolutional layers?    <font color="Darkorange">todo</font></p>
</li>
<li><p>number of parameters</p>
<p>eg. In keras, use <code>model.summary()</code></p>
</li>
</ul>
</li>
</ol>
<h2 id="Representation-Power"><a href="#Representation-Power" class="headerlink" title="Representation Power"></a>Representation Power</h2><blockquote>
<p>Neural networks are <strong>universal function approximators</strong></p>
</blockquote>
<p>For Neural Networks with fully-connected layers, one hidden layer suffices to approximate any function, what’s the point of using more layers and going deeper?</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1feygdqnemxj317y0cigoz.jpg" alt=""></p>
<p>However, for CNN, depth has been found to be an extremely importand component for good recognition system.</p>
<ul>
<li>images contain hierarchical structures, so several layers of processing make intuitive sense for this data domain</li>
</ul>
<h2 id="Setting-number-of-layers-and-their-sizes"><a href="#Setting-number-of-layers-and-their-sizes" class="headerlink" title="Setting number of layers and their sizes"></a>Setting number of layers and their sizes</h2><ul>
<li><p>NN with more neurons can express more complicated functions</p>
<ul>
<li>blessing: learn more complicated data</li>
<li>curse: easier to overfit, but it is better to use other method to prevent overfitting instead of reducing the <code>num_neurons</code></li>
</ul>
</li>
<li><p>Regularization strength is the preferred way to control overfitting of a neural network.</p>
<blockquote>
<p>You should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>
</blockquote>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Theoretical bases and practical advices about NN architecture
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>

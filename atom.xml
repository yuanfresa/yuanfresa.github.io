<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>å¤§è…¿æˆé•¿è®°</title>
  <subtitle>Keep my dream alive, what if it come true?</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yuanfresa.github.io/"/>
  <updated>2017-04-28T15:25:34.000Z</updated>
  <id>http://yuanfresa.github.io/</id>
  
  <author>
    <name>Yuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow å­¦ä¹ ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/uncategorized/TensorFlow-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/uncategorized/TensorFlow-å­¦ä¹ ç¬”è®°/</id>
    <published>2017-04-28T15:25:34.000Z</published>
    <updated>2017-04-28T15:25:34.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part3-Learning-and-Evaluation-æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part3-Learning-and-Evaluation-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-26T20:40:51.000Z</published>
    <updated>2017-04-26T20:42:26.000Z</updated>
    
    <summary type="html">
    
      Useful instructions for training
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-Neural-Network-part2-Setting-up-the-data and the model-æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-26T19:15:35.000Z</published>
    <updated>2017-04-26T20:21:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p><strong>Rules of thumb not laws of the land:</strong> zero-centers and normalizes standard deviations are important for speeding up training</p>
<blockquote>
<p>Computing the mean and subtracting it from every image <em>across the entire dataset</em> and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).</p>
</blockquote>
<p>I think I made a mistake here. Keep it in mind that the mean and the std should be computed only over training data.</p>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>Good methods result in better training.</p>
<h3 id="Weight"><a href="#Weight" class="headerlink" title="Weight"></a>Weight</h3><ul>
<li>HE: Recommended in practice.</li>
</ul>
<ul>
<li>Xavier: Currently use, <font color="Darkorange">todo</font></li>
<li>Derivative of Gaussian: The hidden layers of CNN have an similar effect of computing gradients.</li>
</ul>
<p><a href="Structured Receptive Fields in CNNs">A related paper</a><font color="Darkorange">todo</font></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>We need bias which will increase the flexibility of the model to fit the data.</p>
<p>It is suggested to initialize the biases to be zero.</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><blockquote>
<p>In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. </p>
</blockquote>
<p>After using BN, the speed of convergence increase dramatically. Also it can replace dropout as regularization.<font color="Darkorange">todo</font></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Controlling the capacity of Neural Networks to prevent overfitting.</p>
<p>For regression problem, the loss function, mean squared error(MSE),  works like L2 regularization.</p>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><a href="http://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry/" target="_blank" rel="external">Why L2 loss cause blurry prediction?</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Data-Preprocessing&quot;&gt;&lt;a href=&quot;#Data-Preprocessing&quot; class=&quot;headerlink&quot; title=&quot;Data Preprocessing&quot;&gt;&lt;/a&gt;Data Preprocessing&lt;/h2&gt;&lt;p&gt;&lt;stron
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>hexoå¡«å‘è®°</title>
    <link href="http://yuanfresa.github.io/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/hexo%E5%A1%AB%E5%9D%91%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/ç”Ÿå‘½åœ¨äºæŠ˜è…¾/hexoå¡«å‘è®°/</id>
    <published>2017-04-25T10:18:09.000Z</published>
    <updated>2017-04-25T11:59:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo-github-page-æ­å»º"><a href="#hexo-github-page-æ­å»º" class="headerlink" title="hexo+github page æ­å»º"></a>hexo+github page æ­å»º</h2><p>å››æµ·å…«è’çš„å°ä¼™ä¼´ä»¬åˆ†äº«äº†è¶…å¤šç»éªŒï¼ŒğŸ‘‡è¿™ä¸ªå°±å¾ˆæ£’ã€‚</p>
<p><a href="http://www.tianfeifei.com/2016/08/04/tech/hexo%E5%BB%BA%E7%AB%99/" target="_blank" rel="external">è¯¦ç»†æ•™ç¨‹è¯·æ‰¾æˆ‘</a></p>
<h2 id="Themes"><a href="#Themes" class="headerlink" title="Themes"></a>Themes</h2><p>æŒ‘äº†å¥½ä¹…çš„ä¸»é¢˜ï¼Œæœ‰çš„<a href="https://github.com/yumemor/hexo-theme-varaint" target="_blank" rel="external">å¾ˆå¥½çœ‹</a>ä½†æ˜¯ä¸å¤ªé€‚åˆæŠ€æœ¯åšå®¢ï¼Œä¹Ÿæœ‰çš„<a href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="external">å¾ˆç»å…¸</a>ï¼Œ æœ€åæŒ‘äº†ä¸€ä¸ª<a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="external">å¾ˆç®€å•å¾ˆæœ´ç´ çš„</a></p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez4pwx0cjj31760modis.jpg" alt=""></p>
<h2 id="markdown-å·¥å…·åˆ†äº«-for-mac"><a href="#markdown-å·¥å…·åˆ†äº«-for-mac" class="headerlink" title="markdown å·¥å…·åˆ†äº« for mac"></a>markdown å·¥å…·åˆ†äº« for mac</h2><p>ç¥å¥‡çš„markdownã€‚å¾ˆä¹…ä¹‹å‰ç”¨è¿‡Mouï¼Œä½†æ˜¯å‡çº§æˆSierraä¹‹åå°±ä¸èƒ½ç”¨äº†ï¼Œæ”¹ç”¨äº†macdownï¼Œä½†æ˜¯æœ€è¿‘å‘ç°ä¸€ä¸ªæ›´è…»å®³çš„Typoraã€‚</p>
<ul>
<li>macdownæ¯æ¬¡æ¸²æŸ“éƒ½è‡ªåŠ¨è·³åˆ°æ–‡ç« å‰é¢ï¼Œè¿™ç§ç±»ä¼¼æ–‡ç« çš„ä»£ç å’Œç»“æœåˆ†å¼€çœŸçš„æ˜¯å¥½éš¾ç”¨ã€‚ç¥å™¨æ¥äº†ï¼ŒTyporaï¼Œè¿™ä¹ˆæœ´ç´ ç®€æ´ï¼Œå–œæ¬¢çš„ä¸è¦ä¸è¦çš„ã€‚</li>
<li>macdownæ¯æ¬¡æ¢è¡Œè¦ä¸¤æ¬¡spaceä¸€æ¬¡returnï¼ŒçœŸæ˜¯å¤ªéº»çƒ¦äº†</li>
<li>æ”¯æŒmathï¼Œhighlightï¼Œ è™½ç„¶å‘åˆ°github page å¥½åƒæ²¡å•¥ç”¨ï¼Œä½†æ˜¯ç»™todoliståŠ ä¸€ä¸ªHighlightï¼Œä¹Ÿæ˜¯è›®æ¼‚äº®çš„ã€‚<img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez4wsjcssj30nw0ko0vx.jpg" alt=""></li>
<li>è¿˜æœ‰æ›´å¤šåŠŸèƒ½æœ‰å¾…æŒ–æ˜ã€‚ã€‚ã€‚==todo==</li>
</ul>
<h2 id="æ’å›¾å°å·¥å…·"><a href="#æ’å›¾å°å·¥å…·" class="headerlink" title="æ’å›¾å°å·¥å…·"></a>æ’å›¾å°å·¥å…·</h2><p>markdownæ’ä¸ªå›¾ç‰‡çœŸçš„è´¹åŠ²</p>
<ul>
<li>qqæˆªä¸ªå›¾å­˜åœ¨æ¡Œé¢</li>
<li>æ‰¾ä¸ªç½‘ç«™ä¸Šä¼ å›¾ç‰‡</li>
<li>å¤åˆ¶å›¾ç‰‡åœ°å€</li>
<li>åœ¨Markdownä¸­ä½¿ç”¨<code>![]()</code>è¯­æ³•è°ƒç”¨å›¾ç‰‡æ’å…¥</li>
</ul>
<p>æ¡Œé¢ä¹±ä¸ƒå…«ç³Ÿä¸å¼€å¿ƒï¼Œè€Œä¸”æ¯æ¬¡éƒ½è¦æ‰¾ä¸ªç½‘ç«™ä¸Šä¼ ï¼Œä½†æ˜¯ç»è¿‡å„ä½çŸ¥ä¹å¤§ç¥çš„ä»‹ç»ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€ä¸ªå¾ˆé…·å¾ˆé…·çš„å°å·¥å…·<a href="https://toolinbox.net/iPic/" target="_blank" rel="external">iPic</a>ï¼Œç°åœ¨æ’å›¾çš„è¯ï¼Œåªéœ€è¦</p>
<ul>
<li>æˆªå›¾</li>
<li>ç‚¹ä¸€ä¸‹å³ä¸Šè§’çš„å°å›¾æ ‡ä¸Šä¼ <img src="http://ww3.sinaimg.cn/large/006tKfTcgy1fez564jc1jj301o018glh.jpg" alt=""></li>
<li>è¿˜å¯ä»¥è‡ªåŠ¨ç”Ÿæˆmarkdownæ’å›¾ç‰‡è¯­å¥å“¦</li>
</ul>
<p>ä¸çŸ¥é“ä¼šä¸ä¼šä¸€ç›´å…è´¹ï¼Œå…ˆç”¨ç€å§ã€‚</p>
<h2 id="æ’å…¥æ•°å­¦å…¬å¼"><a href="#æ’å…¥æ•°å­¦å…¬å¼" class="headerlink" title="æ’å…¥æ•°å­¦å…¬å¼"></a>æ’å…¥æ•°å­¦å…¬å¼</h2><p>è™½è¯´Typoraèƒ½å¤Ÿæ¸²æŸ“å‡ºlatexçš„æ•°å­¦å…¬å¼ï¼Œä½†æ˜¯ä¸Šä¼ åˆ°ç½‘é¡µä¸Šå°±ä¸ä¸€å®šæˆåŠŸäº†ï¼Œä¹‹å‰é‚£ä¸ªå¾ˆæ¼‚äº®çš„ä¸»é¢˜å°±æ˜¯ä¸€ç›´å¤±è´¥ï¼Œç»ˆè¢«æˆ‘æŠ›å¼ƒï¼Œ:(</p>
<p>ç½‘ä¸ŠæŸ¥äº†å¾ˆå¤šæ–¹æ³•ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“å“ªä¸ªæœ‰ç”¨ï¼Œå°±éƒ½ç”¨å§ã€‚æˆ‘åšäº†ä»¥ä¸‹è¿™äº›ï¼Œæ˜¯å¯ä»¥æˆåŠŸæ˜¾ç¤ºçš„ã€‚</p>
<h3 id="install-hexo-math"><a href="#install-hexo-math" class="headerlink" title="install hexo-math"></a>install hexo-math</h3><p><code>npm install hexo-math â€”save</code></p>
<h3 id="ä¿®æ”¹-config-yml"><a href="#ä¿®æ”¹-config-yml" class="headerlink" title="ä¿®æ”¹_config.yml"></a>ä¿®æ”¹<code>_config.yml</code></h3><blockquote>
<p>ä¸»ç›®å½•ä¸‹çš„ï¼Œå¦å¤–ä¸»é¢˜ä¸‹ä¹Ÿæœ‰ä¸€ä¸ª_config.ymlï¼Œæ³¨æ„åŒºåˆ†</p>
</blockquote>
<p>åŠ ä¸Š</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># math</div><div class="line">mathjax: true</div></pre></td></tr></table></figure>
<h3 id="æ–‡ç« ä¸­è®¾ç½®"><a href="#æ–‡ç« ä¸­è®¾ç½®" class="headerlink" title="æ–‡ç« ä¸­è®¾ç½®"></a>æ–‡ç« ä¸­è®¾ç½®</h3><p>åœ¨æ–‡ç« éœ€è¦è°ƒç”¨ Mathjax æ—¶, åªéœ€åœ¨ front-matter å‰åŠ ä¸Š <code>mathjax: true</code></p>
<h3 id="latexç”Ÿæˆ"><a href="#latexç”Ÿæˆ" class="headerlink" title="latexç”Ÿæˆ"></a>latexç”Ÿæˆ</h3><p>æœ€åçš„æœ€åç»™å¤§å®¶æ¨èå¥½ç”¨çš„<a href="https://webdemo.myscript.com/views/math.html" target="_blank" rel="external">åœ¨çº¿ç”Ÿæˆlatexå·¥å…·</a>ï¼Œç®€ç›´è¿·å€’äº†ã€‚</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5nzuti2j31kw0p7ag1.jpg" alt=""></p>
<p>æ’å…¥åˆ°æ–‡ç« ä¸­</p>
<figure class="highlight gams"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">$</span>...<span class="symbol">$</span></div><div class="line"><span class="symbol">$</span><span class="symbol">$</span>...<span class="symbol">$</span><span class="symbol">$</span></div></pre></td></tr></table></figure>
<h2 id="è¯„è®ºå·¥å…·"><a href="#è¯„è®ºå·¥å…·" class="headerlink" title="è¯„è®ºå·¥å…·"></a>è¯„è®ºå·¥å…·</h2><p>ç”»é£æ˜¯è¿™ä¸ªæ ·å­çš„</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1fez5rcdd6qj315g09uq3t.jpg" alt=""></p>
<p>ç®€å•çš„æ¥è¯´ï¼Œé¦–å…ˆå»<a href="https://disqus.com/" target="_blank" rel="external">disqus</a>ç”³è¯·ä¸€ä¸ªè´¦å·ï¼Œå–ä¸€ä¸ªæœ‰è¾¨è¯†åº¦çš„shortnameï¼Œç„¶åé“¾æ¥åˆ°åšå®¢ç½‘ç«™ï¼Œæ­¤å¤„ç¥å‘ï¼Œæ¯”å¦‚æˆ‘æƒ³è¦è¿æ¥åˆ°github pageï¼Œä¸€å®šè¦åŠ ä¸Š<code>http://</code>ï¼Œä¸ç„¶åˆ†äº«çš„æ—¶å€™ä¸€ç›´å‡ºç°<code>yoursite.com</code>ç„¶åä¸åŒçš„ä¸»é¢˜æœ‰ä¸åŒçš„è®¾ç½®ï¼Œæˆ‘ç°åœ¨ç”¨çš„è¿™ä¸ªæç®€ä¸»é¢˜ï¼Œåªéœ€è¦åœ¨_config.ymlä¸­åŠ å…¥</p>
<p><code>disqus: yuanfresa ## Your disqus_shortname, e.g. username</code></p>
<h2 id="RSS-sitemap-è®¾ç½®"><a href="#RSS-sitemap-è®¾ç½®" class="headerlink" title="RSS sitemap è®¾ç½®"></a>RSS sitemap è®¾ç½®</h2><p>ç›®å‰ä¸çŸ¥é“æœ‰ä»€ä¹ˆç”¨ï¼Œçœ‹åˆ°è®¸å¤šæ•™ç¨‹éƒ½ç”¨äº†ï¼Œé‚£å’±ä¹Ÿè¯•è¯•ã€‚ä¾æ—§æ˜¯æ¥è‡ªå››æµ·å…«è’çš„å°ä¼™ä¼´çš„åˆ†äº«ã€‚</p>
<p><a href="http://ijiaober.github.io/2014/08/07/hexo/hexo-08/" target="_blank" rel="external">è¯¦ç»†æ•™ç¨‹è¯·ç‚¹æˆ‘</a></p>
<blockquote>
<p>æ³¨æ„ï¼Œæ­¤å¤„æœ‰å‘ï¼Œsitemapå¥½åƒæ˜¯å¯ä»¥ç”Ÿæˆåˆ†äº«çš„é“¾æ¥ï¼Œæ­¤æ—¶ä¸€å®šè¦åœ¨ä¸»ç›®å½•ä¸‹çš„_config.ymlä¸­ä¿®æ”¹urlï¼Œä¸ç„¶é‚£ä¸ªè®¨åŒçš„<code>yoursite.com</code>åˆä¼šå‡ºç°ã€‚</p>
</blockquote>
<p>å¸Œæœ›å¯ä»¥åšæŒæˆ‘çš„å¤§è…¿è®­ç»ƒï¼Œä¹ˆä¹ˆã€‚</p>
]]></content>
    
    <summary type="html">
    
      é‡åˆ°çš„ä¸€äº›å°å‘ï¼Œè¿˜æœ‰å†™åšå®¢å·¥å…·åˆ†äº«
    
    </summary>
    
      <category term="ç”Ÿå‘½åœ¨äºæŠ˜è…¾" scheme="http://yuanfresa.github.io/categories/%E7%94%9F%E5%91%BD%E5%9C%A8%E4%BA%8E%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="hexo" scheme="http://yuanfresa.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Neural Network part1: Setting up the Architecture æ€»ç»“ç¬”è®°</title>
    <link href="http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0/"/>
    <id>http://yuanfresa.github.io/CS231n-notebook/CS231n-Neural-Network-part1-Setting-up-the-Architecture-æ€»ç»“ç¬”è®°/</id>
    <published>2017-04-24T20:55:14.000Z</published>
    <updated>2017-04-24T21:12:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h2><blockquote>
<p>Use the ReLU( $max(x,0)$), be careful with learning rates and possibly monitor the fraction of â€œdeadâ€ units in a network.</p>
</blockquote>
<ol>
<li>Other options</li>
</ol>
<ul>
<li>Leaky ReLU: A small negative slope</li>
</ul>
<p><img src="http://i1.piimg.com/567571/032b87291a258b3f.png" width="300px"></p>
<ul>
<li><p>Maxout: $ max(w_{1}^{T}x+b_{1}, w_{2}^{T}x+b_{2}) $</p>
<p>Special case for ReLU($ w_{1}, b_{1}= 0$) and LeakyReLU<br>(+) linear operation, no saturation<br>(+) no dying<br>(-) double the number of parameters for every neuron  </p>
</li>
</ul>
<ol>
<li><p>Learning rate  </p>
<script type="math/tex; mode=display">error = ReLU(x_{n}) - y</script><script type="math/tex; mode=display">\dfrac {\partial error} {\partial x_{n} }=\delta_{n}=\begin{cases} 1, x_{n}\geq 0\\ 0, x_{n}<0 \end{cases}</script><p>The local gradient of ReLU (which is 1) multiply the gradient that flow-back because of back-propagation, the result of the updated gradient could be a large negative number. Proper learning rate to avoid â€œdeadâ€ zero</p>
</li>
<li><p>Monitor the â€œdeadâ€ units<br>How?   <font color="Darkorange">todo</font></p>
</li>
<li><p>Why sigmoid/tanh out of stage?</p>
</li>
</ol>
<ul>
<li>Saturate and kill gradients</li>
<li>Sigmoid outputs are not zero-centered</li>
</ul>
<h2 id="Neural-Network-architectures"><a href="#Neural-Network-architectures" class="headerlink" title="Neural Network architectures"></a>Neural Network architectures</h2><ol>
<li>How many layers?<ul>
<li>Do not count input layer</li>
</ul>
</li>
</ol>
<ul>
<li>Output layer commonly do not have an activation function</li>
</ul>
<ol>
<li><p>How big the network is?</p>
<ul>
<li><p>number of neurons</p>
<p>Itâ€™s easy to count the fully-connected layers, how about convolutional layers?    <font color="Darkorange">todo</font></p>
</li>
<li><p>number of parameters</p>
<p>eg. In keras, use <code>model.summary()</code></p>
</li>
</ul>
</li>
</ol>
<h2 id="Representation-Power"><a href="#Representation-Power" class="headerlink" title="Representation Power"></a>Representation Power</h2><blockquote>
<p>Neural networks are <strong>universal function approximators</strong></p>
</blockquote>
<p>For Neural Networks with fully-connected layers, one hidden layer suffices to approximate any function, whatâ€™s the point of using more layers and going deeper?</p>
<p><img src="http://ww2.sinaimg.cn/large/006tKfTcgy1feygdqnemxj317y0cigoz.jpg" alt=""></p>
<p>However, for CNN, depth has been found to be an extremely importand component for good recognition system.</p>
<ul>
<li>images contain hierarchical structures, so several layers of processing make intuitive sense for this data domain</li>
</ul>
<h2 id="Setting-number-of-layers-and-their-sizes"><a href="#Setting-number-of-layers-and-their-sizes" class="headerlink" title="Setting number of layers and their sizes"></a>Setting number of layers and their sizes</h2><ul>
<li><p>NN with more neurons can express more complicated functions</p>
<ul>
<li>blessing: learn more complicated data</li>
<li>curse: easier to overfit, but it is better to use other method to prevent overfitting instead of reducing the <code>num_neurons</code></li>
</ul>
</li>
<li><p>Regularization strength is the preferred way to control overfitting of a neural network.</p>
<blockquote>
<p>You should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>
</blockquote>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      Theoretical bases and practical advices about NN architecture
    
    </summary>
    
      <category term="CS231n notebook" scheme="http://yuanfresa.github.io/categories/CS231n-notebook/"/>
    
    
      <category term="deep learning" scheme="http://yuanfresa.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>

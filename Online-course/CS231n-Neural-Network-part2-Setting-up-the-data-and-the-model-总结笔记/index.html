<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay tuned"><title>CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记 | 大腿成长记</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</h1><a id="logo" href="/.">大腿成长记</a><p class="description">Keep my dream alive, what if it come true?</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</h1><div class="post-meta">Apr 26, 2017<span> | </span><span class="category"><a href="/categories/Online-Course/">Online Course</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/" href="/Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Preprocessing"><span class="toc-number">1.</span> <span class="toc-text">Data Preprocessing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Initialization"><span class="toc-number">2.</span> <span class="toc-text">Initialization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Weight"><span class="toc-number">2.1.</span> <span class="toc-text">Weight</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bias"><span class="toc-number">2.2.</span> <span class="toc-text">Bias</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Regularization"><span class="toc-number">4.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss-functions"><span class="toc-number">5.</span> <span class="toc-text">Loss functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification"><span class="toc-number">5.1.</span> <span class="toc-text">Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regression"><span class="toc-number">5.2.</span> <span class="toc-text">Regression</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p><strong>Rules of thumb not laws of the land:</strong> zero-centers and normalizes standard deviations are important for speeding up training</p>
<blockquote>
<p>Computing the mean and subtracting it from every image <em>across the entire dataset</em> and then splitting the data into train/val/test splits would be a mistake. Instead, the mean must be computed only over the training data and then subtracted equally from all splits (train/val/test).</p>
</blockquote>
<p>I think I made a mistake here. Keep it in mind that the mean and the std should be computed only over training data.</p>
<h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>Good methods result in better training.</p>
<h3 id="Weight"><a href="#Weight" class="headerlink" title="Weight"></a>Weight</h3><ul>
<li>HE: Recommended in practice.</li>
</ul>
<ul>
<li>Xavier: Currently use, <font color="Darkorange">todo</font></li>
<li>Derivative of Gaussian: The hidden layers of CNN have an similar effect of computing gradients.</li>
</ul>
<p><a href="Structured Receptive Fields in CNNs">A related paper</a><font color="Darkorange">todo</font></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><p>We need bias which will increase the flexibility of the model to fit the data.</p>
<p>It is suggested to initialize the biases to be zero.</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><blockquote>
<p>In practice networks that use Batch Normalization are significantly more robust to bad initialization. Additionally, batch normalization can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiably manner. </p>
</blockquote>
<p>After using BN, the speed of convergence increase dramatically. Also it can replace dropout as regularization.<font color="Darkorange">todo</font></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Controlling the capacity of Neural Networks to prevent overfitting.</p>
<p>For regression problem, the loss function, mean squared error(MSE),  works like L2 regularization.</p>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p><a href="http://wiseodd.github.io/techblog/2017/02/09/why-l2-blurry/" target="_blank" rel="external">Why L2 loss cause blurry prediction?</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yuanfresa.github.io/Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/" data-id="cj2s2kojl0002ocpnf1hpvjp4" class="article-share-link">Share</a><div class="tags"><a href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post-nav"><a href="/Online-Course/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/" class="pre">CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</a><a href="/生命在于折腾/hexo填坑记/" class="next">hexo填坑记</a></div><div id="disqus_thread"><script>var disqus_shortname = 'yuanfresa';
var disqus_identifier = 'Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/';
var disqus_title = 'CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记';
var disqus_url = 'http://yuanfresa.github.io/Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//yuanfresa.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yuanfresa.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Online-Course/">Online Course</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Papers/">Papers</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Practice-Coding/">Practice Coding</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/生命在于折腾/">生命在于折腾</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Coding/" style="font-size: 15px;">Coding</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Online-Course/Programming-Foundations-with-Python/">Programming Foundations with Python</a></li><li class="post-list-item"><a class="post-list-link" href="/Practice-Coding/Codility-Test/">Codility Test</a></li><li class="post-list-item"><a class="post-list-link" href="/生命在于折腾/Learning-Google-Python-Style/">Learning Google Python Style</a></li><li class="post-list-item"><a class="post-list-link" href="/TensorFlow/TensorFlow-学习笔记-2/">TensorFlow-学习笔记(2)</a></li><li class="post-list-item"><a class="post-list-link" href="/TensorFlow/Preprocessing-the-Data/">Preprocessing the Data</a></li><li class="post-list-item"><a class="post-list-link" href="/Papers/关于-Batch-Normalization/">关于 Batch Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/TensorFlow/TensorFlow-学习笔记-1/">TensorFlow 学习笔记(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/Online-Course/CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记/">CS231n-Neural-Network-part3-Learning-and-Evaluation-总结笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/Online-Course/CS231n-Neural-Network-part2-Setting-up-the-data-and-the-model-总结笔记/">CS231n-Neural-Network-part2-Setting-up-the-data and the model-总结笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/生命在于折腾/hexo填坑记/">hexo填坑记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//yuanfresa.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/yuanfresa" title="github" target="_blank">github</a><ul></ul><a href="https://www.zhihu.com/people/yumemor" title="zhihu" target="_blank">zhihu</a><ul></ul><a href="https://twitter.com/Yuan_Fresa/" title="twitter" target="_blank">twitter</a><ul></ul><a href="https://www.facebook.com/yuan.zhou.31392" title="facebook" target="_blank">facebook</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a>2017 </a><a href="/." rel="nofollow">大腿成长记.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>